{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "from __future__ import division, unicode_literals \n",
    "from urllib import request\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "from nltk.tokenize import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define reusable functions\n",
    "\n",
    "def read_html(url):\n",
    "    html = request.urlopen(url).read().decode('utf8')\n",
    "    raw = BeautifulSoup(html, 'html.parser').get_text()\n",
    "    tokens = word_tokenize(raw)\n",
    "    text = nltk.Text(tokens)\n",
    "    return text\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    return len(set(text))/len(text)\n",
    "\n",
    "def percentage(word,text):\n",
    "    return 100* text.count(word)/len(text)\n",
    "\n",
    "def read_utf8(url):\n",
    "    response = request.urlopen(url)\n",
    "    raw = response.read().decode('utf8')\n",
    "    tokens = nltk.word_tokenize(raw)\n",
    "    text = nltk.Text(tokens)\n",
    "    return text\n",
    "\n",
    "def vocab(tokens):\n",
    "    return set(tokens)\n",
    "\n",
    "def read_utf8_contentonly(url):\n",
    "    response = request.urlopen(url)\n",
    "    raw = response.read().decode('utf8')\n",
    "    start=raw.find(\"CONTENTS\")\n",
    "    if start== -1:\n",
    "        start=0\n",
    "    #print(start)    \n",
    "    if raw.rfind(\"End of the Project Gutenberg\") != -1:\n",
    "        end=raw.rfind(\"End of the Project Gutenberg\")\n",
    "    elif raw.rfind(\"End of Project Gutenberg's\") != -1:\n",
    "        end=raw.rfind(\"End of Project Gutenberg's\")\n",
    "    \n",
    "    if end == -1:\n",
    "        end=len(raw)\n",
    "        \n",
    "    #print(end)\n",
    "    tokens = nltk.word_tokenize(raw[start:end])\n",
    "    text = nltk.Text(tokens)\n",
    "    return text\n",
    "\n",
    "\n",
    "def text_norm(text):\n",
    "    #remove numbers and punctuations and convert all words to lower case to remove duplicates later\n",
    "    words=[word.lower() for word in text if word.isalpha()]\n",
    "    return words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-analyze ebook list\n",
    "\n",
    "url = \"http://www.gutenberg.org/cache/epub/9078/pg9078.txt\"\n",
    "response = request.urlopen(url)\n",
    "#print(response.status, response.reason)\n",
    "# good output is 200,OK\n",
    "raw = response.read().decode('utf8')\n",
    "tokens = word_tokenize(raw)\n",
    "text = nltk.Text(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "684852"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)\n",
    "len(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5542\n",
      "671148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "140591"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk=read_utf8_contentonly(\"http://www.gutenberg.org/cache/epub/9078/pg9078.txt\")\n",
    "len(tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿Project\n",
      "Gutenberg\n",
      "'s\n",
      "Sanders\n",
      "'\n",
      "Union\n",
      "Fourth\n",
      "Reader\n",
      ",\n",
      "by\n",
      "144160\n",
      "<class 'list'>\n",
      "144160\n",
      "<class 'nltk.text.Text'>\n"
     ]
    }
   ],
   "source": [
    "# print tokens/text\n",
    "#tokens is a list of strings\n",
    "for t in range(0,10):\n",
    "    print(text[t])\n",
    "    \n",
    "print(len(tokens))\n",
    "print(type(tokens))\n",
    "\n",
    "print(len(text))\n",
    "print(type(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17120\n",
      "<class 'set'>\n",
      "100 lamp\n",
      "101 ILS\n",
      "102 licks\n",
      "103 eventide\n",
      "104 HOLLAND\n",
      "105 cur_v'd_\n",
      "106 concave\n",
      "107 When\n",
      "108 _his_\n",
      "109 recovery\n",
      "110 _sp_\n"
     ]
    }
   ],
   "source": [
    "## Create vocabulary\n",
    "vocab=set(tokens)\n",
    "print(len(vocab))\n",
    "print(type(vocab))\n",
    "# print set objects\n",
    "start=100\n",
    "for count, item in enumerate(vocab, start):\n",
    "    print(count, item)\n",
    "    if(count==start+10):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second reader (The Beacon Second Reader BookIcon.png Fassett, James H.)\n",
    "#text[0]=read_html(\"http://www.gutenberg.org/files/15659/15659-h/15659-h.htm\")\n",
    "#fifth reader (De La Salle Fifth Reader BookIcon.png Schools, Brothers of the Christian)\n",
    "#text[1]=read_html(\"http://www.gutenberg.org/files/10811/10811-h/10811-h.htm\")\n",
    "#fourth reader (Sanders' Union Fourth Reader BookIcon.png Sanders, Charles W.)\n",
    "#text[2]=read_html(\"http://www.gutenberg.org/files/9078/9078-h/9078-h.htm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define reusable functions for list of books\n",
    "\n",
    "def createlist(booklist,norm):\n",
    "    textread=[]\n",
    "    docnames=[]\n",
    "    if norm:\n",
    "        for name,url in booklist.items():\n",
    "            docnames.append(name)\n",
    "            textread.append(text_norm(read_utf8_contentonly(url)))\n",
    "    else:\n",
    "        for name,url in booklist.items():\n",
    "            docnames.append(name)\n",
    "            textread.append(read_utf8(url))\n",
    "    \n",
    "    return docnames,textread\n",
    "\n",
    "# find lexical diversity of the books\n",
    "def lexdiv_list(textread):\n",
    "    lexdiv=[]\n",
    "    for tmp in textread:\n",
    "        lexdiv.append(lexical_diversity(tmp))\n",
    "    #for l in lexdiv:\n",
    "    #    print(l)\n",
    "    \n",
    "    return lexdiv\n",
    "\n",
    "# find vocab of books\n",
    "def vocabsize_list(textread):\n",
    "    vocabsize=[]\n",
    "    for tmp in textread:\n",
    "        vocabsize.append(len(vocab(tmp)))\n",
    "    #for vc in vocabsize:\n",
    "    #    print(vc)\n",
    "    \n",
    "    return vocabsize\n",
    "    \n",
    "# print tabular output\n",
    "def tab_bookstats(book_pd):\n",
    "    print(tabulate(book_pd, headers='keys', tablefmt='simple',showindex=False,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Lexical Diversity and Vocabulary Size of set of 3 books at a time from different grades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexical diversity and Vocabulary size on text that is not normalized/cleaned and from books of different authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docnames                                                                       lexical_diversity    vocabulary_size\n",
      "---------------------------------------------------------------------------  -------------------  -----------------\n",
      "New National First Reader : Barnes, Charles J.                                          0.165179               2202\n",
      "The Ontario Readers: Third Book : Ontario Ministry of Education                         0.120272               9877\n",
      "fifth reader :De La Salle Fifth Reader : Schools, Brothers of the Christian             0.129179               9196\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "diff_author_docs={\"second reader :The Beacon Second Reader : Fassett, James H.\" : 'http://www.gutenberg.org/cache/epub/15659/pg15659.txt',\n",
    "                  \"fourth reader :Sanders' Union Fourth Reader : Sanders, Charles W.\" : 'http://www.gutenberg.org/cache/epub/9078/pg9078.txt',\n",
    "                  \"fifth reader :De La Salle Fifth Reader : Schools, Brothers of the Christian\" :'http://www.gutenberg.org/cache/epub/10811/pg10811.txt'}\n",
    "'''\n",
    "\n",
    "diff_author_docs={\"New National First Reader : Barnes, Charles J.\":'http://www.gutenberg.org/files/13853/13853-0.txt',\n",
    "                  \"The Ontario Readers: Third Book : Ontario Ministry of Education\":'http://www.gutenberg.org/cache/epub/18561/pg18561.txt',\n",
    "                  \"fifth reader :De La Salle Fifth Reader : Schools, Brothers of the Christian\" :'http://www.gutenberg.org/cache/epub/10811/pg10811.txt'}\n",
    "                  \n",
    "da_dcname,da_textr = createlist(diff_author_docs,False)\n",
    "da_ld=lexdiv_list(da_textr)\n",
    "da_vc=vocabsize_list(da_textr)\n",
    "da_df = pd.DataFrame({'docnames' : da_dcname,\n",
    "                   'lexical_diversity' : da_ld,\n",
    "                   'vocabulary_size' : da_vc})\n",
    "\n",
    "tab_bookstats(da_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note_ : Inferences and analysis is purely based on samples chosen for this analysis and it could differ based on more samples.\n",
    "\n",
    "At first randomly selected 3 books of different grades from different authors. It is assumed Different authors have different style of writing and may use different vocabulary as well.  From the above chosen sample, we can see that vocabulary size and lexical diversity are inversely proportional i.e. with increasing vocabulary_size, lexical diversity is decreasing, propertion rate being different. Although it cannot be said that with rising grade the vocabulary was increasing, which we can say from above samples as vocabulary size of 3rd grade bood was greater than 5th grade book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexical diversity and Vocabulary size on text that is not normalized/cleaned and from books of same author\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docnames                                                                        lexical_diversity    vocabulary_size\n",
      "----------------------------------------------------------------------------  -------------------  -----------------\n",
      "McGuffey's First Eclectic Reader, Revised Edition : McGuffey, William Holmes             0.167199               2147\n",
      "McGuffey's Third Eclectic Reader : McGuffey, William Holmes                              0.124138               4714\n",
      "McGuffey's Fifth Eclectic Reader : McGuffey, William Holmes                              0.112887              14292\n"
     ]
    }
   ],
   "source": [
    "same_author_docs={\"McGuffey's First Eclectic Reader, Revised Edition : McGuffey, William Holmes\" : 'http://www.gutenberg.org/cache/epub/14640/pg14640.txt',\n",
    "                  \"McGuffey's Third Eclectic Reader : McGuffey, William Holmes\" : 'http://www.gutenberg.org/cache/epub/14766/pg14766.txt',\n",
    "                  \"McGuffey's Fifth Eclectic Reader : McGuffey, William Holmes\" :'http://www.gutenberg.org/cache/epub/15040/pg15040.txt'}\n",
    "\n",
    "sa_dcname,sa_textr = createlist(same_author_docs,False)\n",
    "sa_ld=lexdiv_list(sa_textr)\n",
    "sa_vc=vocabsize_list(sa_textr)\n",
    "sa_df = pd.DataFrame({'docnames' : sa_dcname,\n",
    "                   'lexical_diversity' : sa_ld,\n",
    "                   'vocabulary_size' : sa_vc})\n",
    "tab_bookstats(sa_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note_ : Inferences and analysis is purely based on samples chosen for this analysis and it could differ based on more samples.\n",
    "\n",
    "At first randomly selected 3 books of different grades from different authors. It is assumed Different authors have different style of writing and may use different vocabulary as well. So here we have picked up books from different grades but form the same author to see if authors prefer to change pattern in the vocabulary with rising grade. Well from the chosen samples, vocabulary size is inversely proportional to lexical diversity here also. However the vocabulary size is increasing with rising grade, propertion rate being different between books of 2 sonsecutive grades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Previous 2 samples showed a inverse relation between lexical diversity and vocabulary size. However, the data chosen had lot of non-text information as well and it would be interesting to see if considering a cleaner text gives us different answers. For this purpose some cleaning is performed on the text read from books.** \n",
    "\n",
    "**1) Mostly different books and different authors may structure their books differently. On manual inspection of the ebooks on gutenerg site, it was found that most of these books have header and footer information of book comprising of information on author, licenses, publications and project gutenberg.There was a different title of the actual start of book, so \"CONTENT\" followed by Table of contents is considered to be start of book. It does repeat some information but we will be able to remove lot of other information like Prefaces,Acknowledgements , etc which have more data.**  \n",
    "**2) Similarly, to consider the end of book, it was seen that most of books had a sequence of words related to \"End of the Project Gutenberg\" , after which lot of information about this project is mentioned.**  \n",
    "**3) Also to count the vocabulary, punctuations and numerical data has been removed**\n",
    "\n",
    "#### Lexical diversity and Vocabulary size on text that is normalized/cleaned and from books of different authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docnames                                                                       lexical_diversity    vocabulary_size\n",
      "---------------------------------------------------------------------------  -------------------  -----------------\n",
      "New National First Reader : Barnes, Charles J.                                          0.114483                796\n",
      "The Ontario Readers: Third Book : Ontario Ministry of Education                         0.117208               7397\n",
      "fifth reader :De La Salle Fifth Reader : Schools, Brothers of the Christian             0.127714               6701\n"
     ]
    }
   ],
   "source": [
    "              \n",
    "dan_dcname,dan_textr = createlist(diff_author_docs,True)\n",
    "#print(dan_dcname)\n",
    "#print(dan_textr)\n",
    "dan_ld=lexdiv_list(dan_textr)\n",
    "dan_vc=vocabsize_list(dan_textr)\n",
    "dan_df = pd.DataFrame({'docnames' : dan_dcname,\n",
    "                   'lexical_diversity' : dan_ld,\n",
    "                   'vocabulary_size' : dan_vc})\n",
    "tab_bookstats(dan_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note_ : Inferences and analysis is purely based on samples chosen for this analysis and it could differ based on more samples.\n",
    "\n",
    "This normalized data clearly shows the relation between lexical diversity and vocabulary size has changed now for the same set of books from different authors. We do not see a pattern of inverse relationship , also interesting to find that although vocabulary size from Grade 1 to Grade 3 has increased significantly( 796 to 7397 ) , lexical diversity seems to have changed slightly ( 0.1144 to 0.1172 ) . While the vocabulary size from Grade 3 to Grade 5 changed by a comparatively higher number, from 7397 to 6701, lexical diversity has increased with a higher proportion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexical diversity and Vocabulary size on text that is  normalized/cleaned and from books of same author\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docnames                                                                        lexical_diversity    vocabulary_size\n",
      "----------------------------------------------------------------------------  -------------------  -----------------\n",
      "McGuffey's First Eclectic Reader, Revised Edition : McGuffey, William Holmes             0.151628               1136\n",
      "McGuffey's Third Eclectic Reader : McGuffey, William Holmes                              0.123868               3200\n",
      "McGuffey's Fifth Eclectic Reader : McGuffey, William Holmes                              0.111936              10571\n"
     ]
    }
   ],
   "source": [
    "san_dcname,san_textr = createlist(same_author_docs,True)\n",
    "san_ld=lexdiv_list(san_textr)\n",
    "san_vc=vocabsize_list(san_textr)\n",
    "san_df = pd.DataFrame({'docnames' : san_dcname,\n",
    "                   'lexical_diversity' : san_ld,\n",
    "                   'vocabulary_size' : san_vc})\n",
    "tab_bookstats(san_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note_ : Inferences and analysis is purely based on samples chosen for this analysis and it could differ based on more samples.\n",
    "\n",
    "This normalized data clearly shows the relation between lexical diversity and vocabulary size has unchanged for the same set of books from same author. We see similar pattern of increasing vocabulary size with rising grade and inverse relation between lexical diversity and vocabulary size. Although it is interesting to see that vocabulary size has decreased significantly when we considered text normalization/cleanup , lexical diversity has not changed significantly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above text analysis on different sets of data it seems more appropriate to consider the statistics from cleaned up data. Not because they do not confirm a pattern but logically even though it was a basic cleanup, it should more closely represent the data contents of the book. And not just that there is no relation between the 2 statistics, the rate of change of these statistics considered individually is different . So the text complexity should be analyzed using both the scores jointly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
