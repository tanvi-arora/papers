{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 4\n",
    "\n",
    "**Submitted by : Tanvi Arora**   \n",
    "**Section     : DS 7337 Natural Language Processing - 401**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "### Contents\n",
    "\n",
    "* <a href=\"#functions\">Function Definitions</a>\n",
    "* <a href=\"#corpus\">Corpus and Data prep</a>\n",
    "* <a href=\"#patternpos\">1 - POS Tagger in Pattern library</a>\n",
    "* <a href=\"#patternposlong\">1.a - Longest sentence using pattern POS tagger</a>\n",
    "* <a href=\"#patternposshort\">1.b - Shortest sentence using pattern POS tagger</a>\n",
    "* <a href=\"#spacypos\">2 -POS Tagger in spaCy library</a>\n",
    "* <a href=\"#spacyposlong\">Longest sentence using spaCy POS tagger</a>\n",
    "* <a href=\"#spacyposshort\">Shortest sentence using spaCy POS tagger</a>\n",
    "* <a href=\"#comppatternvsspacy\">2.a - Compare pattern POS vs spaCy POS</a>\n",
    "* <a href=\"#exppatternvsspacy\">2.b - Explain pattern POS vs spaCy POS</a>\n",
    "* <a href=\"#news\">3 - News article</a>\n",
    "* <a href=\"#newsmanual\">3.a - Manually POS POS Tag using Penn Tagset</a>\n",
    "* <a href=\"#newscomppatternvsspacy\">3.b - Compare manual POS vs Pattern POS vs spaCy POS</a>\n",
    "* <a href=\"#newsexppatternvsspacy\">3.c - Explain manual POS vs Pattern POS vs spaCy POS</a>\n",
    "* <a href=\"#addtagers\">Additional Taggers : ClassiferBasedPOSTagger</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Darwin-18.5.0-x86_64-i386-64bit\n",
      "environment base\n",
      "Python 3.7.3 (default, Mar 27 2019, 16:54:48) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "nltk 3.4\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.platform())\n",
    "\n",
    "import os\n",
    "print (\"environment\",os.environ['CONDA_DEFAULT_ENV'])\n",
    "\n",
    "import sys\n",
    "print(\"Python\",sys.version)\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "print(\"nltk\",nltk.__version__)\n",
    "\n",
    "## corpus referred\n",
    "from nltk.corpus import brown\n",
    "\n",
    "from nltk.tag.util import tuple2str\n",
    "\n",
    "## for visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "## pattern library\n",
    "from pattern.en import tag\n",
    "from pattern.text import UNIVERSAL\n",
    "\n",
    "## spaCy library\n",
    "import spacy\n",
    "sp_nlp=spacy.load('en')\n",
    "from spacy import displacy\n",
    "\n",
    "pd.set_option('display.max_rows',10)\n",
    "import re\n",
    "\n",
    "## ignore/suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "\n",
    "# To display plots inside the iPython Notebook itself\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach followed**  \n",
    "1. create tagged words using your POS tagger  \n",
    "2. validate POS tag  :\n",
    "    -- chk length , if length is not equal then strip punctuations  \n",
    "    -- if length still does nto match, then output not matching POS tag  \n",
    "    -- if length matches compare the 2 POS tags.  \n",
    "    -- if len(match) <> len of inputs then POS tags are not matching. output string tuple  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"functions\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "\n",
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pattern universal tagset had same number of tags but different names ( used 2 char names)\n",
    "## Brown corpus uses more than 3 char names for universal tagset\n",
    "## Below dictionary is to convert pattern tags to match brown corpus tag names\n",
    "pt_universal_tags={\"NN\":\"NOUN\",\n",
    "               \"VB\":\"VERB\",\n",
    "                \"JJ\":\"ADJ\",\n",
    "                \"RB\":\"ADV\",\n",
    "                \"PR\" : \"PRON\",\n",
    "                \"DT\" :\"DET\",\n",
    "                \"PP\" :\"PREP\",\n",
    "                \"NO\":\"NUM\" ,\n",
    "                \"CJ\":\"CONJ\" ,\n",
    "                \"UH\" :\"INTJ\",\n",
    "                \"PT\":\"PRT\" ,\n",
    "                \".\" : \".\",\n",
    "                \"X\"  :\"X\"}\n",
    "\n",
    "## spaCy universal tagset uses Google Universal dataset which is Universal dataset with a few additions\n",
    "## Below dictionary is to convert spaCy tags to match brown corpus tag names. Names missing in brown corpus \n",
    "## will be added with same names as in spaCy , they would not match \n",
    "sp_universal_tags={\"NOUN\":\"NOUN\",\n",
    "               \"VERB\":\"VERB\",\n",
    "                \"ADJ\":\"ADJ\",\n",
    "                \"ADP\":\"ADP\",\n",
    "                \"ADV\":\"ADV\",\n",
    "                \"AUX\":\"AUX\",\n",
    "                \"PRON\" : \"PRON\",\n",
    "                \"DET\" :\"DET\",\n",
    "                \"SCONJ\":\"SCONJ\",\n",
    "                \"CONJ\":\"CONJ\" ,\n",
    "                \"CCONJ\":\"CCONJ\",\n",
    "                \"INTJ\" :\"INTJ\",\n",
    "                \"PART\":\"PRT\" ,\n",
    "                \"PUNCT\"  :\".\",\n",
    "                \"X\"  :\"X\",\n",
    "                \"NUM\":\"NUM\",\n",
    "                \"PROPN\":\"PROPN\",\n",
    "                \"SYM\":\".\"}\n",
    "\n",
    "## Below function will convert tokens to a sentence.\n",
    "## brown corpus tagged_sents returns a list of sentence-tokens\n",
    "def tokens2sent(tokens):\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "## removes punctions from the list of tagged words\n",
    "def remove_punct(tagged_word):\n",
    "    return [(tag[0],tag[1]) for tag in tagged_word if tag[0].isalpha()]\n",
    "\n",
    "## This function prints the tagged words in the form of list ( word,tag) as word/tag and joins them with a space\n",
    "## to form a sentene type output\n",
    "def print_tup2str(tagged_word):\n",
    "    tagged_str=[tuple2str(t1) for t1 in tagged_word]\n",
    "    return ' '.join(tagged_str)\n",
    "\n",
    "## Display output of comparision as a message with the input and output\n",
    "def print_output(verdict,golden_tag,pos_tag,pos_tag_name):\n",
    "    if(verdict):\n",
    "        print(\"POS tag is correct and matches the golden tag\")\n",
    "    else:\n",
    "        print(\"POS tag may not be correct as it does not matches the golden tag\")\n",
    "    print(\"golden tag ---:\")\n",
    "    print(print_tup2str(golden_tag))\n",
    "    print(\"POS tag ---\", pos_tag_name, \":\")\n",
    "    print(print_tup2str(pos_tag))\n",
    "\n",
    "## Compare function that takes a golden key set and pos tagger keyset\n",
    "def compare_pos(golden_tag,pos_tag,pos_tag_name,display):\n",
    "    #if len(golden_tag)!=len(pos_tag):\n",
    "        #golden_tag=remove_punct(golden_tag)\n",
    "        #pos_tag=remove_punct(pos_tag)\n",
    "    if len(golden_tag)!=len(pos_tag):\n",
    "        verdict=False\n",
    "    else:\n",
    "        match=[i for i,j in zip(golden_tag,pos_tag) if i==j]\n",
    "        if len(match)==len(golden_tag):\n",
    "            verdict=True\n",
    "        else:\n",
    "            verdict=False\n",
    "    if(display):\n",
    "        print_output(verdict,golden_tag,pos_tag,pos_tag_name)\n",
    "    return verdict\n",
    "\n",
    "## apply POS Tagger from Pattern using UNIVERSAL tagset\n",
    "## in case the tagset has any combination tags, strip anything after first hyphen\n",
    "def pos_pattern(sentence):\n",
    "    pattern_tag=tag(sentence,tagset=UNIVERSAL)\n",
    "    return [(t[0],pt_universal_tags[t[1].split(\"-\",1)[0]]) for t in pattern_tag]\n",
    "\n",
    "## apply POS tagger from spaCy using UNIVERSAL tagset\n",
    "def pos_spacy(sentence):\n",
    "    return [(token.text,sp_universal_tags[token.pos_]) for token in sp_nlp(sentence)]\n",
    "\n",
    "## apply POS Tagger from Pattern using default/ Penn Tag Tagset\n",
    "## in case the tagset has any combination tags, strip anything after first hyphen\n",
    "def pos_pattern_penn(sentence):\n",
    "    pattern_penn_tag=tag(sentence)\n",
    "    return [(t[0],t[1].split(\"-\",1)[0]) for t in pattern_penn_tag]\n",
    "\n",
    "\n",
    "## apply POS Tagger from spaCy using default/ Penn Tag Tagset\n",
    "def pos_spacy_penn(sentence):\n",
    "    return [(token.text,token.tag_) for token in sp_nlp(sentence)]\n",
    "\n",
    "## returns pattern POS tags for the sentence at a given index along with its comparision with golden tagset\n",
    "def pos_pattern_disp_byindex(ind,golden_tag_lst,input_df,pos_tag_name):\n",
    "    pattern_pos_ind=pos_pattern(input_df.sentence.iloc[ind])\n",
    "    brown_golden_ind=golden_tag_lst[ind]\n",
    "    print(\"original sentence --- :\")\n",
    "    print(input_df.sentence.iloc[ind])\n",
    "    compare_pos(brown_golden_ind,pattern_pos_ind,pos_tag_name,True)\n",
    "\n",
    "## returns spaCy POS tags for the sentence at a given index along with its comparision with golden tagset\n",
    "def pos_spacy_disp_byindex(ind,golden_tag_lst,input_df,pos_tag_name):\n",
    "    spacy_pos_ind=pos_spacy(input_df.sentence.iloc[ind])\n",
    "    brown_golden_ind=golden_tag_lst[ind]\n",
    "    print(\"original sentence --- :\")\n",
    "    print(input_df.sentence.iloc[ind])\n",
    "    compare_pos(brown_golden_ind,spacy_pos_ind,pos_tag_name,True)\n",
    "    \n",
    "## returns pattern POS tags for the sentence at a given index along with its comparision with golden tagset\n",
    "def pos_pattern_penn_disp_byindex(ind,golden_tag_lst,input_df,pos_tag_name):\n",
    "    pattern_penn_pos_ind=pos_pattern_penn(input_df.sentence.iloc[ind])\n",
    "    brown_golden_ind=golden_tag_lst[ind]\n",
    "    print(\"original sentence --- :\")\n",
    "    print(input_df.sentence.iloc[ind])\n",
    "    compare_pos(brown_golden_ind,pattern_penn_pos_ind,pos_tag_name,True)\n",
    "\n",
    "## returns spaCy POS tags for the sentence at a given index along with its comparision with golden tagset\n",
    "def pos_spacy_penn_disp_byindex(ind,golden_tag_lst,input_df,pos_tag_name):\n",
    "    spacy_penn_pos_ind=pos_spacy_penn(input_df.sentence.iloc[ind])\n",
    "    brown_golden_ind=golden_tag_lst[ind]\n",
    "    print(\"original sentence --- :\")\n",
    "    print(input_df.sentence.iloc[ind])\n",
    "    compare_pos(brown_golden_ind,spacy_penn_pos_ind,pos_tag_name,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"corpus\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "### About the Corpus under study - Brown Corpus\n",
    "\n",
    "The Brown University Standard Corpus of Present-Day American English (or just Brown Corpus) was compiled in the 1960s by Henry Kučera and W. Nelson Francis at Brown University, Providence, Rhode Island as a general corpus (text collection) in the field of corpus linguistics. It contains 500 samples of English-language text, totaling roughly one million words, compiled from works published in the United States in 1961.\n",
    "\n",
    "The tagged Brown Corpus used a selection of about 80 parts of speech, as well as special indicators for compound forms, contractions, foreign words and a few other phenomena, and formed the basis for many later corpora such as the Lancaster-Oslo-Bergen Corpus. Some versions of the tagged Brown corpus contain combined tags. To perform auto compare of the POS tags returned by other Taggers, we have used the Universal tagset which is a shorter tagset of around 12 tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNIVERSAL POS tags\n",
    "\n",
    "Alphabetical listing  \n",
    "\n",
    "ADJ: adjective  \n",
    "ADP: adposition  \n",
    "ADV: adverb  \n",
    "AUX: auxiliary  \n",
    "CCONJ: coordinating conjunction  \n",
    "DET: determiner  \n",
    "INTJ: interjection  \n",
    "NOUN: noun  \n",
    "NUM: numeral  \n",
    "PART: particle  \n",
    "PRON: pronoun  \n",
    "PROPN: proper noun  \n",
    "PUNCT: punctuation  \n",
    "SCONJ: subordinating conjunction  \n",
    "SYM: symbol  \n",
    "VERB: verb  \n",
    "X: other  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider Brown corpus tagged words as the golden tagset for comparision\n",
    "brown_goldentag=brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe using brown corpus sentences , with length of each sentence( token list)\n",
    "# which will be further used for comparisions\n",
    "sent_dyn=[(c,len(value),value) for c,value in enumerate(brown.sents(),0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57340"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Brown corpus has 57340 sentences in total from all categories\n",
    "len(sent_dyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>length</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>[The, Fulton, County, Grand, Jury, said, Frida...</td>\n",
       "      <td>The Fulton County Grand Jury said Friday an in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>[The, jury, further, said, in, term-end, prese...</td>\n",
       "      <td>The jury further said in term-end presentments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>[The, September-October, term, jury, had, been...</td>\n",
       "      <td>The September-October term jury had been charg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>[``, Only, a, relative, handful, of, such, rep...</td>\n",
       "      <td>`` Only a relative handful of such reports was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>[The, jury, said, it, did, find, that, many, o...</td>\n",
       "      <td>The jury said it did find that many of Georgia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   position  length                                             tokens  \\\n",
       "0         0      25  [The, Fulton, County, Grand, Jury, said, Frida...   \n",
       "1         1      43  [The, jury, further, said, in, term-end, prese...   \n",
       "2         2      35  [The, September-October, term, jury, had, been...   \n",
       "3         3      37  [``, Only, a, relative, handful, of, such, rep...   \n",
       "4         4      24  [The, jury, said, it, did, find, that, many, o...   \n",
       "\n",
       "                                            sentence  \n",
       "0  The Fulton County Grand Jury said Friday an in...  \n",
       "1  The jury further said in term-end presentments...  \n",
       "2  The September-October term jury had been charg...  \n",
       "3  `` Only a relative handful of such reports was...  \n",
       "4  The jury said it did find that many of Georgia...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_df=pd.DataFrame(sent_dyn)\n",
    "sent_df.columns=[\"position\",\"length\",\"tokens\"]\n",
    "sent_df[\"sentence\"]=[tokens2sent(t) for t in sent_df[\"tokens\"]]\n",
    "sent_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>length</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28302</th>\n",
       "      <td>28302</td>\n",
       "      <td>180</td>\n",
       "      <td>[and, (, C, ), to, finance, ,, for, not, more,...</td>\n",
       "      <td>and ( C ) to finance , for not more than three...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45083</th>\n",
       "      <td>45083</td>\n",
       "      <td>172</td>\n",
       "      <td>[He, sucked, in, his, breath, and, kept, quiet...</td>\n",
       "      <td>He sucked in his breath and kept quiet while K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13174</th>\n",
       "      <td>13174</td>\n",
       "      <td>161</td>\n",
       "      <td>[``, We, ,, the, Subscribers, ,, do, agree, ,,...</td>\n",
       "      <td>`` We , the Subscribers , do agree , that as s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26764</th>\n",
       "      <td>26764</td>\n",
       "      <td>146</td>\n",
       "      <td>[There, is, ,, of, course, ,, nothing, new, ab...</td>\n",
       "      <td>There is , of course , nothing new about dysto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29382</th>\n",
       "      <td>29382</td>\n",
       "      <td>145</td>\n",
       "      <td>[Inventory, and, evaluate, wildlife, habitat, ...</td>\n",
       "      <td>Inventory and evaluate wildlife habitat resour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4823</th>\n",
       "      <td>4823</td>\n",
       "      <td>1</td>\n",
       "      <td>[.]</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33331</th>\n",
       "      <td>33331</td>\n",
       "      <td>1</td>\n",
       "      <td>[Summary]</td>\n",
       "      <td>Summary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24598</th>\n",
       "      <td>24598</td>\n",
       "      <td>1</td>\n",
       "      <td>['']</td>\n",
       "      <td>''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48418</th>\n",
       "      <td>48418</td>\n",
       "      <td>1</td>\n",
       "      <td>['']</td>\n",
       "      <td>''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28194</th>\n",
       "      <td>28194</td>\n",
       "      <td>1</td>\n",
       "      <td>[Governor]</td>\n",
       "      <td>Governor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57340 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       position  length                                             tokens  \\\n",
       "28302     28302     180  [and, (, C, ), to, finance, ,, for, not, more,...   \n",
       "45083     45083     172  [He, sucked, in, his, breath, and, kept, quiet...   \n",
       "13174     13174     161  [``, We, ,, the, Subscribers, ,, do, agree, ,,...   \n",
       "26764     26764     146  [There, is, ,, of, course, ,, nothing, new, ab...   \n",
       "29382     29382     145  [Inventory, and, evaluate, wildlife, habitat, ...   \n",
       "...         ...     ...                                                ...   \n",
       "4823       4823       1                                                [.]   \n",
       "33331     33331       1                                          [Summary]   \n",
       "24598     24598       1                                               ['']   \n",
       "48418     48418       1                                               ['']   \n",
       "28194     28194       1                                         [Governor]   \n",
       "\n",
       "                                                sentence  \n",
       "28302  and ( C ) to finance , for not more than three...  \n",
       "45083  He sucked in his breath and kept quiet while K...  \n",
       "13174  `` We , the Subscribers , do agree , that as s...  \n",
       "26764  There is , of course , nothing new about dysto...  \n",
       "29382  Inventory and evaluate wildlife habitat resour...  \n",
       "...                                                  ...  \n",
       "4823                                                   .  \n",
       "33331                                            Summary  \n",
       "24598                                                 ''  \n",
       "48418                                                 ''  \n",
       "28194                                           Governor  \n",
       "\n",
       "[57340 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_df.sort_values(by=['length'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**The longest sentence Brown corpus has has 180 words and shortest is of 1 word, which looks like has only punctuations sometimes**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"patternpos\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "### PATTERN POS Tagger\n",
    "\n",
    "Apply **Pattern POS Tagger** to all the sentences in Brown corpus for further study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_pos=[]\n",
    "result_pt_pos=[]\n",
    "for index in range(0,len(sent_df)):\n",
    "    #print(\"index :\",index)\n",
    "    s_p=pos_pattern(sent_df.sentence.iloc[index])\n",
    "    #print(s_p)\n",
    "    pattern_pos.append(s_p)\n",
    "    result_pt_pos.append(compare_pos(brown_goldentag[sent_df.position.iloc[index]],s_p,'pattern_pos',False))\n",
    "\n",
    "sent_df['pattern_pos_tagger']=result_pt_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>length</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sentence</th>\n",
       "      <th>pattern_pos_tagger</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1862</th>\n",
       "      <td>1862</td>\n",
       "      <td>52</td>\n",
       "      <td>[Also, Mrs., Berton, Korman, ,, Mrs., Morton, ...</td>\n",
       "      <td>Also Mrs. Berton Korman , Mrs. Morton Rosen , ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27794</th>\n",
       "      <td>27794</td>\n",
       "      <td>40</td>\n",
       "      <td>[It, embraced, determining, when, to, purchase...</td>\n",
       "      <td>It embraced determining when to purchase and w...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28141</th>\n",
       "      <td>28141</td>\n",
       "      <td>32</td>\n",
       "      <td>[These, three, installment, dates, would, be, ...</td>\n",
       "      <td>These three installment dates would be : Octob...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15042</th>\n",
       "      <td>15042</td>\n",
       "      <td>29</td>\n",
       "      <td>[Here, may, be, found, regular, and, impact, s...</td>\n",
       "      <td>Here may be found regular and impact styrene ,...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45184</th>\n",
       "      <td>45184</td>\n",
       "      <td>28</td>\n",
       "      <td>[I, knew, the, only, way, I, could, beat, you,...</td>\n",
       "      <td>I knew the only way I could beat you was to pl...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7196</th>\n",
       "      <td>7196</td>\n",
       "      <td>1</td>\n",
       "      <td>[)]</td>\n",
       "      <td>)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6887</th>\n",
       "      <td>6887</td>\n",
       "      <td>1</td>\n",
       "      <td>[.]</td>\n",
       "      <td>.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6849</th>\n",
       "      <td>6849</td>\n",
       "      <td>1</td>\n",
       "      <td>[Vacancy]</td>\n",
       "      <td>Vacancy</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27049</th>\n",
       "      <td>27049</td>\n",
       "      <td>1</td>\n",
       "      <td>[)]</td>\n",
       "      <td>)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47840</th>\n",
       "      <td>47840</td>\n",
       "      <td>1</td>\n",
       "      <td>[Attack]</td>\n",
       "      <td>Attack</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4646 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       position  length                                             tokens  \\\n",
       "1862       1862      52  [Also, Mrs., Berton, Korman, ,, Mrs., Morton, ...   \n",
       "27794     27794      40  [It, embraced, determining, when, to, purchase...   \n",
       "28141     28141      32  [These, three, installment, dates, would, be, ...   \n",
       "15042     15042      29  [Here, may, be, found, regular, and, impact, s...   \n",
       "45184     45184      28  [I, knew, the, only, way, I, could, beat, you,...   \n",
       "...         ...     ...                                                ...   \n",
       "7196       7196       1                                                [)]   \n",
       "6887       6887       1                                                [.]   \n",
       "6849       6849       1                                          [Vacancy]   \n",
       "27049     27049       1                                                [)]   \n",
       "47840     47840       1                                           [Attack]   \n",
       "\n",
       "                                                sentence  pattern_pos_tagger  \n",
       "1862   Also Mrs. Berton Korman , Mrs. Morton Rosen , ...                True  \n",
       "27794  It embraced determining when to purchase and w...                True  \n",
       "28141  These three installment dates would be : Octob...                True  \n",
       "15042  Here may be found regular and impact styrene ,...                True  \n",
       "45184  I knew the only way I could beat you was to pl...                True  \n",
       "...                                                  ...                 ...  \n",
       "7196                                                   )                True  \n",
       "6887                                                   .                True  \n",
       "6849                                             Vacancy                True  \n",
       "27049                                                  )                True  \n",
       "47840                                             Attack                True  \n",
       "\n",
       "[4646 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_df[sent_df['pattern_pos_tagger']==True].sort_values(by=['length'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**Based on the comparisions of Pattern POS tags and the brown corpus tags, 4646 sentences in total had a match out of 57340 sentences. The longest word with a matching POS tag is 52 words long and shortest is 1 word long**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"patternposlong\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "\n",
    "<span style=\"color:blue\">**Longest sentence correctly tagged , when compared to Brown corpus tags, by Pattern POS tagger**</span>\n",
    "\n",
    "original sentence -- is the input  \n",
    "golden tag -- is the brown corpus tag  \n",
    "POS tag -- pattern_pos -- is the tags returned by pattern POS , modified using the dictionary to match the namesin brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original sentence --- :\n",
      "Also Mrs. Berton Korman , Mrs. Morton Rosen , Mrs. Jacques Zinman , Mrs. Evelyn Rosen , Mrs. Henry Schultz , Mr. and Mrs. I. S. Kamens , Mrs. Jack Langsdorf , Mrs. Leonard Liss , Mrs. Gordon Blumberg , Mrs. Oscar Bregman , Mrs. Alfred Kershbaum and Mrs. Edward Sabol .\n",
      "POS tag is correct and matches the golden tag\n",
      "golden tag ---:\n",
      "Also/ADV Mrs./NOUN Berton/NOUN Korman/NOUN ,/. Mrs./NOUN Morton/NOUN Rosen/NOUN ,/. Mrs./NOUN Jacques/NOUN Zinman/NOUN ,/. Mrs./NOUN Evelyn/NOUN Rosen/NOUN ,/. Mrs./NOUN Henry/NOUN Schultz/NOUN ,/. Mr./NOUN and/CONJ Mrs./NOUN I./NOUN S./NOUN Kamens/NOUN ,/. Mrs./NOUN Jack/NOUN Langsdorf/NOUN ,/. Mrs./NOUN Leonard/NOUN Liss/NOUN ,/. Mrs./NOUN Gordon/NOUN Blumberg/NOUN ,/. Mrs./NOUN Oscar/NOUN Bregman/NOUN ,/. Mrs./NOUN Alfred/NOUN Kershbaum/NOUN and/CONJ Mrs./NOUN Edward/NOUN Sabol/NOUN ./.\n",
      "POS tag --- pattern_pos :\n",
      "Also/ADV Mrs./NOUN Berton/NOUN Korman/NOUN ,/. Mrs./NOUN Morton/NOUN Rosen/NOUN ,/. Mrs./NOUN Jacques/NOUN Zinman/NOUN ,/. Mrs./NOUN Evelyn/NOUN Rosen/NOUN ,/. Mrs./NOUN Henry/NOUN Schultz/NOUN ,/. Mr./NOUN and/CONJ Mrs./NOUN I./NOUN S./NOUN Kamens/NOUN ,/. Mrs./NOUN Jack/NOUN Langsdorf/NOUN ,/. Mrs./NOUN Leonard/NOUN Liss/NOUN ,/. Mrs./NOUN Gordon/NOUN Blumberg/NOUN ,/. Mrs./NOUN Oscar/NOUN Bregman/NOUN ,/. Mrs./NOUN Alfred/NOUN Kershbaum/NOUN and/CONJ Mrs./NOUN Edward/NOUN Sabol/NOUN ./.\n"
     ]
    }
   ],
   "source": [
    "pos_pattern_disp_byindex(1862,brown_goldentag,sent_df,'pattern_pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"patternposshort\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "\n",
    "<span style=\"color:blue\">**Shortest sentence incorrectly tagged, when compared to Brown corpus tags, by Pattern POS tagger**</span>\n",
    "\n",
    "We have seen sentences with 1 words , often containing punctuations only. And these have been tagged incorrectly for some. Although these may be valid sentences, usually a sentence should have atleast 1 verb and 1 noun , that makes it 2 words per sentence. For our analysis we will consider 2 word sentences as the minimum length of sentence to check the incorrectly tagged sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>length</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sentence</th>\n",
       "      <th>pattern_pos_tagger</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27609</th>\n",
       "      <td>27609</td>\n",
       "      <td>2</td>\n",
       "      <td>[2, .]</td>\n",
       "      <td>2 .</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36029</th>\n",
       "      <td>36029</td>\n",
       "      <td>2</td>\n",
       "      <td>[4, .]</td>\n",
       "      <td>4 .</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35692</th>\n",
       "      <td>35692</td>\n",
       "      <td>2</td>\n",
       "      <td>[Electric, power]</td>\n",
       "      <td>Electric power</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51224</th>\n",
       "      <td>51224</td>\n",
       "      <td>2</td>\n",
       "      <td>[Strange, .]</td>\n",
       "      <td>Strange .</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3249</th>\n",
       "      <td>3249</td>\n",
       "      <td>2</td>\n",
       "      <td>[Murphy, honors]</td>\n",
       "      <td>Murphy honors</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38730</th>\n",
       "      <td>38730</td>\n",
       "      <td>9</td>\n",
       "      <td>[It, was, still, Good, Friday, ,, after, all, .]</td>\n",
       "      <td>It was still Good Friday , after all .</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50362</th>\n",
       "      <td>50362</td>\n",
       "      <td>9</td>\n",
       "      <td>[He, took, a, long, but, carefully, controlled...</td>\n",
       "      <td>He took a long but carefully controlled draught .</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50361</th>\n",
       "      <td>50361</td>\n",
       "      <td>9</td>\n",
       "      <td>[He, cleansed, his, mouth, with, a, small, qua...</td>\n",
       "      <td>He cleansed his mouth with a small quantity .</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38556</th>\n",
       "      <td>38556</td>\n",
       "      <td>9</td>\n",
       "      <td>[He, waved, his, arm, around, at, the, furnish...</td>\n",
       "      <td>He waved his arm around at the furnishings .</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6126</th>\n",
       "      <td>6126</td>\n",
       "      <td>9</td>\n",
       "      <td>[In, 1952, Mr., Eisenhower, won, all, but, Mis...</td>\n",
       "      <td>In 1952 Mr. Eisenhower won all but Missouri .</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7980 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       position  length                                             tokens  \\\n",
       "27609     27609       2                                             [2, .]   \n",
       "36029     36029       2                                             [4, .]   \n",
       "35692     35692       2                                  [Electric, power]   \n",
       "51224     51224       2                                       [Strange, .]   \n",
       "3249       3249       2                                   [Murphy, honors]   \n",
       "...         ...     ...                                                ...   \n",
       "38730     38730       9   [It, was, still, Good, Friday, ,, after, all, .]   \n",
       "50362     50362       9  [He, took, a, long, but, carefully, controlled...   \n",
       "50361     50361       9  [He, cleansed, his, mouth, with, a, small, qua...   \n",
       "38556     38556       9  [He, waved, his, arm, around, at, the, furnish...   \n",
       "6126       6126       9  [In, 1952, Mr., Eisenhower, won, all, but, Mis...   \n",
       "\n",
       "                                                sentence  pattern_pos_tagger  \n",
       "27609                                                2 .               False  \n",
       "36029                                                4 .               False  \n",
       "35692                                     Electric power               False  \n",
       "51224                                          Strange .               False  \n",
       "3249                                       Murphy honors               False  \n",
       "...                                                  ...                 ...  \n",
       "38730             It was still Good Friday , after all .               False  \n",
       "50362  He took a long but carefully controlled draught .               False  \n",
       "50361      He cleansed his mouth with a small quantity .               False  \n",
       "38556       He waved his arm around at the furnishings .               False  \n",
       "6126       In 1952 Mr. Eisenhower won all but Missouri .               False  \n",
       "\n",
       "[7980 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sent_df[(sent_df['pattern_pos_tagger']==False) & (sent_df['length']<10) &(sent_df['length']>1)].sort_values(by=['length'],ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original sentence --- :\n",
      "Electric power\n",
      "POS tag may not be correct as it does not matches the golden tag\n",
      "golden tag ---:\n",
      "Electric/ADJ power/NOUN\n",
      "POS tag --- pattern_pos :\n",
      "Electric/NOUN power/NOUN\n"
     ]
    }
   ],
   "source": [
    "pos_pattern_disp_byindex(35692,brown_goldentag,sent_df,'pattern_pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**Above is the tags of a 2 word sentence that was incorrectly tagged. As per Brown corpus Eating facilities , has a adjective and noun, but our Pattern POS tagger, tags both the words as Noun. \n",
    "Brown Corpus tags were created by members of the humanistic world in various academic institutions, including Brown University.**\n",
    "Comparing other similar 2 word sentences, brown corpus has wither a combination of verb-noun, adj-noun, adv-noun but pattern has identified these as noun-noun.</span>\n",
    "\n",
    "Let's look  at some of the longer sentences that are incorrectly tagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>length</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sentence</th>\n",
       "      <th>pattern_pos_tagger</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>187</td>\n",
       "      <td>5</td>\n",
       "      <td>[``, Must, solve, problem, '']</td>\n",
       "      <td>`` Must solve problem ''</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>981</td>\n",
       "      <td>5</td>\n",
       "      <td>[--, emphasizes, the, Virgin, birth]</td>\n",
       "      <td>-- emphasizes the Virgin birth</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>1004</td>\n",
       "      <td>5</td>\n",
       "      <td>[', church, meets, change, ']</td>\n",
       "      <td>' church meets change '</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>1069</td>\n",
       "      <td>5</td>\n",
       "      <td>[Seeks, ``, improved, fielding, '']</td>\n",
       "      <td>Seeks `` improved fielding ''</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>1074</td>\n",
       "      <td>5</td>\n",
       "      <td>[Duren, ,, Sheldon, on, hill]</td>\n",
       "      <td>Duren , Sheldon on hill</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57059</th>\n",
       "      <td>57059</td>\n",
       "      <td>5</td>\n",
       "      <td>[It, wasn't, very, funny, .]</td>\n",
       "      <td>It wasn't very funny .</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57066</th>\n",
       "      <td>57066</td>\n",
       "      <td>5</td>\n",
       "      <td>[That, helped, a, lot, .]</td>\n",
       "      <td>That helped a lot .</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57077</th>\n",
       "      <td>57077</td>\n",
       "      <td>5</td>\n",
       "      <td>[We, couldn't, help, laughing, .]</td>\n",
       "      <td>We couldn't help laughing .</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57144</th>\n",
       "      <td>57144</td>\n",
       "      <td>5</td>\n",
       "      <td>[``, We, don't, '', .]</td>\n",
       "      <td>`` We don't '' .</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57246</th>\n",
       "      <td>57246</td>\n",
       "      <td>5</td>\n",
       "      <td>[Far, from, it, ;, ;]</td>\n",
       "      <td>Far from it ; ;</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>857 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       position  length                                tokens  \\\n",
       "187         187       5        [``, Must, solve, problem, '']   \n",
       "981         981       5  [--, emphasizes, the, Virgin, birth]   \n",
       "1004       1004       5         [', church, meets, change, ']   \n",
       "1069       1069       5   [Seeks, ``, improved, fielding, '']   \n",
       "1074       1074       5         [Duren, ,, Sheldon, on, hill]   \n",
       "...         ...     ...                                   ...   \n",
       "57059     57059       5          [It, wasn't, very, funny, .]   \n",
       "57066     57066       5             [That, helped, a, lot, .]   \n",
       "57077     57077       5     [We, couldn't, help, laughing, .]   \n",
       "57144     57144       5                [``, We, don't, '', .]   \n",
       "57246     57246       5                 [Far, from, it, ;, ;]   \n",
       "\n",
       "                             sentence  pattern_pos_tagger  \n",
       "187          `` Must solve problem ''               False  \n",
       "981    -- emphasizes the Virgin birth               False  \n",
       "1004          ' church meets change '               False  \n",
       "1069    Seeks `` improved fielding ''               False  \n",
       "1074          Duren , Sheldon on hill               False  \n",
       "...                               ...                 ...  \n",
       "57059          It wasn't very funny .               False  \n",
       "57066             That helped a lot .               False  \n",
       "57077     We couldn't help laughing .               False  \n",
       "57144                `` We don't '' .               False  \n",
       "57246                 Far from it ; ;               False  \n",
       "\n",
       "[857 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_rows = 10\n",
    "sent_df[(sent_df['pattern_pos_tagger']==False) & (sent_df['length']==5)].sort_values(by=['position'],ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original sentence --- :\n",
      "We couldn't help laughing .\n",
      "POS tag may not be correct as it does not matches the golden tag\n",
      "golden tag ---:\n",
      "We/PRON couldn't/VERB help/VERB laughing/VERB ./.\n",
      "POS tag --- pattern_pos :\n",
      "We/PRON could/VERB n't/ADV help/VERB laughing/VERB ./.\n"
     ]
    }
   ],
   "source": [
    "pos_pattern_disp_byindex(57077,brown_goldentag,sent_df,'pattern_pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">From the incorrectly tagged sentences, picking up a sentence to analyze that has shorthands. And see how it is tagged ?\n",
    "As we can see above, the difference is due to how pattern tokenizes the word couldn't. Brown corpus identifies it as a single word, \"couldn't\" but pattern breaks it as \"could\" and \"n't\" . Rest of the words have been tagged correctly.  \n",
    "_Pattern.en module 's tag function, accepts a string and internally calls a tokenize function which returns a list of sentences , with punctuations marks split from words._</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('could', 'VERB'), (\"n't\", 'ADV')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_pattern(\"couldn't\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"spacypos\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "### Using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_pos=[]\n",
    "result_sp_pos=[]\n",
    "for index in range(0,len(sent_df)):\n",
    "    #print(\"index :\",index)\n",
    "    s_s=pos_spacy(sent_df.sentence.iloc[index])\n",
    "    #print(s_p)\n",
    "    spacy_pos.append(s_s)\n",
    "    result_sp_pos.append(compare_pos(brown_goldentag[sent_df.position.iloc[index]],s_s,'spacy_pos',False))\n",
    "\n",
    "sent_df['spacy_pos_tagger']=result_sp_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"spacyposlong\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "\n",
    "**Longest sentence correctly matched, when compared to Brown corpus tags, returned by spaCy POS tagger**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>length</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sentence</th>\n",
       "      <th>pattern_pos_tagger</th>\n",
       "      <th>spacy_pos_tagger</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32265</th>\n",
       "      <td>32265</td>\n",
       "      <td>57</td>\n",
       "      <td>[In, repetitions, of, the, experiment, from, c...</td>\n",
       "      <td>In repetitions of the experiment from couple t...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33410</th>\n",
       "      <td>33410</td>\n",
       "      <td>51</td>\n",
       "      <td>[A, borderline, schizophrenic, young, man, tol...</td>\n",
       "      <td>A borderline schizophrenic young man told me t...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32732</th>\n",
       "      <td>32732</td>\n",
       "      <td>51</td>\n",
       "      <td>[Of, startling, significance, ,, too, ,, is, t...</td>\n",
       "      <td>Of startling significance , too , is the asser...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15980</th>\n",
       "      <td>15980</td>\n",
       "      <td>46</td>\n",
       "      <td>[Extensive, observations, by, physicians, duri...</td>\n",
       "      <td>Extensive observations by physicians during va...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18083</th>\n",
       "      <td>18083</td>\n",
       "      <td>46</td>\n",
       "      <td>[The, best, rule, of, thumb, for, detecting, c...</td>\n",
       "      <td>The best rule of thumb for detecting corked wi...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26598</th>\n",
       "      <td>26598</td>\n",
       "      <td>1</td>\n",
       "      <td>[8]</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27049</th>\n",
       "      <td>27049</td>\n",
       "      <td>1</td>\n",
       "      <td>[)]</td>\n",
       "      <td>)</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27373</th>\n",
       "      <td>27373</td>\n",
       "      <td>1</td>\n",
       "      <td>[Contact]</td>\n",
       "      <td>Contact</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27440</th>\n",
       "      <td>27440</td>\n",
       "      <td>1</td>\n",
       "      <td>[Loans]</td>\n",
       "      <td>Loans</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27511</th>\n",
       "      <td>27511</td>\n",
       "      <td>1</td>\n",
       "      <td>[Repayment]</td>\n",
       "      <td>Repayment</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8978 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       position  length                                             tokens  \\\n",
       "32265     32265      57  [In, repetitions, of, the, experiment, from, c...   \n",
       "33410     33410      51  [A, borderline, schizophrenic, young, man, tol...   \n",
       "32732     32732      51  [Of, startling, significance, ,, too, ,, is, t...   \n",
       "15980     15980      46  [Extensive, observations, by, physicians, duri...   \n",
       "18083     18083      46  [The, best, rule, of, thumb, for, detecting, c...   \n",
       "...         ...     ...                                                ...   \n",
       "26598     26598       1                                                [8]   \n",
       "27049     27049       1                                                [)]   \n",
       "27373     27373       1                                          [Contact]   \n",
       "27440     27440       1                                            [Loans]   \n",
       "27511     27511       1                                        [Repayment]   \n",
       "\n",
       "                                                sentence  pattern_pos_tagger  \\\n",
       "32265  In repetitions of the experiment from couple t...               False   \n",
       "33410  A borderline schizophrenic young man told me t...               False   \n",
       "32732  Of startling significance , too , is the asser...               False   \n",
       "15980  Extensive observations by physicians during va...               False   \n",
       "18083  The best rule of thumb for detecting corked wi...               False   \n",
       "...                                                  ...                 ...   \n",
       "26598                                                  8                True   \n",
       "27049                                                  )                True   \n",
       "27373                                            Contact                True   \n",
       "27440                                              Loans                True   \n",
       "27511                                          Repayment                True   \n",
       "\n",
       "       spacy_pos_tagger  \n",
       "32265              True  \n",
       "33410              True  \n",
       "32732              True  \n",
       "15980              True  \n",
       "18083              True  \n",
       "...                 ...  \n",
       "26598              True  \n",
       "27049              True  \n",
       "27373              True  \n",
       "27440              True  \n",
       "27511              True  \n",
       "\n",
       "[8978 rows x 6 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_df[sent_df['spacy_pos_tagger']==True].sort_values(by=['length'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**spaCy POS Tagger returned 8978 tags matching the Brown corpus tags**</span>\n",
    "\n",
    "**Longest sentence that was correctly matched by spaCy POS Tagger , when compared to Brown corpus, is 57 words long,\n",
    "shortest being 1 word long**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original sentence --- :\n",
      "In repetitions of the experiment from couple to couple , the votes of the two persons in a couple probably agree more often than independence would imply , because couples who visit the museum together are more likely to have similar tastes than are a random pair of people drawn from the entire population of visitors .\n",
      "POS tag is correct and matches the golden tag\n",
      "golden tag ---:\n",
      "In/ADP repetitions/NOUN of/ADP the/DET experiment/NOUN from/ADP couple/NOUN to/ADP couple/NOUN ,/. the/DET votes/NOUN of/ADP the/DET two/NUM persons/NOUN in/ADP a/DET couple/NOUN probably/ADV agree/VERB more/ADV often/ADV than/ADP independence/NOUN would/VERB imply/VERB ,/. because/ADP couples/NOUN who/PRON visit/VERB the/DET museum/NOUN together/ADV are/VERB more/ADV likely/ADJ to/PRT have/VERB similar/ADJ tastes/NOUN than/ADP are/VERB a/DET random/ADJ pair/NOUN of/ADP people/NOUN drawn/VERB from/ADP the/DET entire/ADJ population/NOUN of/ADP visitors/NOUN ./.\n",
      "POS tag --- spacy_pos :\n",
      "In/ADP repetitions/NOUN of/ADP the/DET experiment/NOUN from/ADP couple/NOUN to/ADP couple/NOUN ,/. the/DET votes/NOUN of/ADP the/DET two/NUM persons/NOUN in/ADP a/DET couple/NOUN probably/ADV agree/VERB more/ADV often/ADV than/ADP independence/NOUN would/VERB imply/VERB ,/. because/ADP couples/NOUN who/PRON visit/VERB the/DET museum/NOUN together/ADV are/VERB more/ADV likely/ADJ to/PRT have/VERB similar/ADJ tastes/NOUN than/ADP are/VERB a/DET random/ADJ pair/NOUN of/ADP people/NOUN drawn/VERB from/ADP the/DET entire/ADJ population/NOUN of/ADP visitors/NOUN ./.\n"
     ]
    }
   ],
   "source": [
    "pos_spacy_disp_byindex(32265,brown_goldentag,sent_df,'spacy_pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"spacyposshort\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "\n",
    "**Shortest sentence correctly matched, when compared to Brown corpus tags, returned by spaCy POS tagger**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>length</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sentence</th>\n",
       "      <th>pattern_pos_tagger</th>\n",
       "      <th>spacy_pos_tagger</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30828</th>\n",
       "      <td>30828</td>\n",
       "      <td>2</td>\n",
       "      <td>[3, .]</td>\n",
       "      <td>3 .</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34812</th>\n",
       "      <td>34812</td>\n",
       "      <td>2</td>\n",
       "      <td>[5, .]</td>\n",
       "      <td>5 .</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34816</th>\n",
       "      <td>34816</td>\n",
       "      <td>2</td>\n",
       "      <td>[6, .]</td>\n",
       "      <td>6 .</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34824</th>\n",
       "      <td>34824</td>\n",
       "      <td>2</td>\n",
       "      <td>[1, .]</td>\n",
       "      <td>1 .</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34830</th>\n",
       "      <td>34830</td>\n",
       "      <td>2</td>\n",
       "      <td>[2, .]</td>\n",
       "      <td>2 .</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38481</th>\n",
       "      <td>38481</td>\n",
       "      <td>9</td>\n",
       "      <td>[It, was, left, out, of, him, at, birth, .]</td>\n",
       "      <td>It was left out of him at birth .</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50932</th>\n",
       "      <td>50932</td>\n",
       "      <td>9</td>\n",
       "      <td>[I've, been, that, far, half, a, dozen, times, .]</td>\n",
       "      <td>I've been that far half a dozen times .</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38503</th>\n",
       "      <td>38503</td>\n",
       "      <td>9</td>\n",
       "      <td>[Jane, asked, in, her, placid, ,, interested, ...</td>\n",
       "      <td>Jane asked in her placid , interested way .</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50995</th>\n",
       "      <td>50995</td>\n",
       "      <td>9</td>\n",
       "      <td>[``, Yes, '', ,, Herr, Schaffner, had, said, .]</td>\n",
       "      <td>`` Yes '' , Herr Schaffner had said .</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42546</th>\n",
       "      <td>42546</td>\n",
       "      <td>9</td>\n",
       "      <td>[Can, you, find, it, all, right, '', ?, ?]</td>\n",
       "      <td>Can you find it all right '' ? ?</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7367 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       position  length                                             tokens  \\\n",
       "30828     30828       2                                             [3, .]   \n",
       "34812     34812       2                                             [5, .]   \n",
       "34816     34816       2                                             [6, .]   \n",
       "34824     34824       2                                             [1, .]   \n",
       "34830     34830       2                                             [2, .]   \n",
       "...         ...     ...                                                ...   \n",
       "38481     38481       9        [It, was, left, out, of, him, at, birth, .]   \n",
       "50932     50932       9  [I've, been, that, far, half, a, dozen, times, .]   \n",
       "38503     38503       9  [Jane, asked, in, her, placid, ,, interested, ...   \n",
       "50995     50995       9    [``, Yes, '', ,, Herr, Schaffner, had, said, .]   \n",
       "42546     42546       9         [Can, you, find, it, all, right, '', ?, ?]   \n",
       "\n",
       "                                          sentence  pattern_pos_tagger  \\\n",
       "30828                                          3 .                True   \n",
       "34812                                          5 .                True   \n",
       "34816                                          6 .                True   \n",
       "34824                                          1 .                True   \n",
       "34830                                          2 .               False   \n",
       "...                                            ...                 ...   \n",
       "38481            It was left out of him at birth .               False   \n",
       "50932      I've been that far half a dozen times .               False   \n",
       "38503  Jane asked in her placid , interested way .               False   \n",
       "50995        `` Yes '' , Herr Schaffner had said .               False   \n",
       "42546             Can you find it all right '' ? ?               False   \n",
       "\n",
       "       spacy_pos_tagger  \n",
       "30828             False  \n",
       "34812             False  \n",
       "34816             False  \n",
       "34824             False  \n",
       "34830             False  \n",
       "...                 ...  \n",
       "38481             False  \n",
       "50932             False  \n",
       "38503             False  \n",
       "50995             False  \n",
       "42546             False  \n",
       "\n",
       "[7367 rows x 6 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_rows = 10\n",
    "sent_df[(sent_df['spacy_pos_tagger']==False) & (sent_df['length']<10) &(sent_df['length']>1)].sort_values(by=['length'],ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>length</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sentence</th>\n",
       "      <th>pattern_pos_tagger</th>\n",
       "      <th>spacy_pos_tagger</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>187</td>\n",
       "      <td>5</td>\n",
       "      <td>[``, Must, solve, problem, '']</td>\n",
       "      <td>`` Must solve problem ''</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>676</td>\n",
       "      <td>5</td>\n",
       "      <td>[Formula, is, due, this, week]</td>\n",
       "      <td>Formula is due this week</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>962</td>\n",
       "      <td>5</td>\n",
       "      <td>[Oak, Grove, (, special, )]</td>\n",
       "      <td>Oak Grove ( special )</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>981</td>\n",
       "      <td>5</td>\n",
       "      <td>[--, emphasizes, the, Virgin, birth]</td>\n",
       "      <td>-- emphasizes the Virgin birth</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>1069</td>\n",
       "      <td>5</td>\n",
       "      <td>[Seeks, ``, improved, fielding, '']</td>\n",
       "      <td>Seeks `` improved fielding ''</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57066</th>\n",
       "      <td>57066</td>\n",
       "      <td>5</td>\n",
       "      <td>[That, helped, a, lot, .]</td>\n",
       "      <td>That helped a lot .</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57077</th>\n",
       "      <td>57077</td>\n",
       "      <td>5</td>\n",
       "      <td>[We, couldn't, help, laughing, .]</td>\n",
       "      <td>We couldn't help laughing .</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57110</th>\n",
       "      <td>57110</td>\n",
       "      <td>5</td>\n",
       "      <td>[Arlene, became, indispensable, ;, ;]</td>\n",
       "      <td>Arlene became indispensable ; ;</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57144</th>\n",
       "      <td>57144</td>\n",
       "      <td>5</td>\n",
       "      <td>[``, We, don't, '', .]</td>\n",
       "      <td>`` We don't '' .</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57168</th>\n",
       "      <td>57168</td>\n",
       "      <td>5</td>\n",
       "      <td>[Mr., Gorboduc, looked, down, .]</td>\n",
       "      <td>Mr. Gorboduc looked down .</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>801 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       position  length                                 tokens  \\\n",
       "187         187       5         [``, Must, solve, problem, '']   \n",
       "676         676       5         [Formula, is, due, this, week]   \n",
       "962         962       5            [Oak, Grove, (, special, )]   \n",
       "981         981       5   [--, emphasizes, the, Virgin, birth]   \n",
       "1069       1069       5    [Seeks, ``, improved, fielding, '']   \n",
       "...         ...     ...                                    ...   \n",
       "57066     57066       5              [That, helped, a, lot, .]   \n",
       "57077     57077       5      [We, couldn't, help, laughing, .]   \n",
       "57110     57110       5  [Arlene, became, indispensable, ;, ;]   \n",
       "57144     57144       5                 [``, We, don't, '', .]   \n",
       "57168     57168       5       [Mr., Gorboduc, looked, down, .]   \n",
       "\n",
       "                              sentence  pattern_pos_tagger  spacy_pos_tagger  \n",
       "187           `` Must solve problem ''               False             False  \n",
       "676           Formula is due this week                True             False  \n",
       "962              Oak Grove ( special )                True             False  \n",
       "981     -- emphasizes the Virgin birth               False             False  \n",
       "1069     Seeks `` improved fielding ''               False             False  \n",
       "...                                ...                 ...               ...  \n",
       "57066              That helped a lot .               False             False  \n",
       "57077      We couldn't help laughing .               False             False  \n",
       "57110  Arlene became indispensable ; ;                True             False  \n",
       "57144                 `` We don't '' .               False             False  \n",
       "57168       Mr. Gorboduc looked down .                True             False  \n",
       "\n",
       "[801 rows x 6 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_df[(sent_df['spacy_pos_tagger']==False) & (sent_df['length']==5)].sort_values(by=['position'],ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original sentence --- :\n",
      "Electric power\n",
      "POS tag is correct and matches the golden tag\n",
      "golden tag ---:\n",
      "Electric/ADJ power/NOUN\n",
      "POS tag --- spacy_pos :\n",
      "Electric/ADJ power/NOUN\n"
     ]
    }
   ],
   "source": [
    "pos_spacy_disp_byindex(35692,brown_goldentag,sent_df,'spacy_pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original sentence --- :\n",
      "We couldn't help laughing .\n",
      "POS tag may not be correct as it does not matches the golden tag\n",
      "golden tag ---:\n",
      "We/PRON couldn't/VERB help/VERB laughing/VERB ./.\n",
      "POS tag --- spacy_pos :\n",
      "We/PRON could/AUX n't/ADV help/VERB laughing/VERB ./.\n"
     ]
    }
   ],
   "source": [
    "pos_spacy_disp_byindex(57077,brown_goldentag,sent_df,'spacy_pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comppatternvsspacy\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "\n",
    "### Compare pattern POS tagger with spaCy POS tagger "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's look at the sentences we identified from Pattern POS tagger as the longest matching and shorter not matching**\n",
    "\n",
    "Consider pattern tag as the golden tag now and compare it with the tags returned by spacy\n",
    "\n",
    "**_Longest correct sentence at index# 1862_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tag may not be correct as it does not matches the golden tag\n",
      "golden tag ---:\n",
      "Also/ADV Mrs./NOUN Berton/NOUN Korman/NOUN ,/. Mrs./NOUN Morton/NOUN Rosen/NOUN ,/. Mrs./NOUN Jacques/NOUN Zinman/NOUN ,/. Mrs./NOUN Evelyn/NOUN Rosen/NOUN ,/. Mrs./NOUN Henry/NOUN Schultz/NOUN ,/. Mr./NOUN and/CONJ Mrs./NOUN I./NOUN S./NOUN Kamens/NOUN ,/. Mrs./NOUN Jack/NOUN Langsdorf/NOUN ,/. Mrs./NOUN Leonard/NOUN Liss/NOUN ,/. Mrs./NOUN Gordon/NOUN Blumberg/NOUN ,/. Mrs./NOUN Oscar/NOUN Bregman/NOUN ,/. Mrs./NOUN Alfred/NOUN Kershbaum/NOUN and/CONJ Mrs./NOUN Edward/NOUN Sabol/NOUN ./.\n",
      "POS tag --- patternVsspacy :\n",
      "Also/ADV Mrs./PROPN Berton/PROPN Korman/PROPN ,/. Mrs./PROPN Morton/PROPN Rosen/PROPN ,/. Mrs./PROPN Jacques/PROPN Zinman/PROPN ,/. Mrs./PROPN Evelyn/PROPN Rosen/PROPN ,/. Mrs./PROPN Henry/PROPN Schultz/PROPN ,/. Mr./PROPN and/CCONJ Mrs./PROPN I./PROPN S./PROPN Kamens/PROPN ,/. Mrs./PROPN Jack/PROPN Langsdorf/PROPN ,/. Mrs./PROPN Leonard/PROPN Liss/PROPN ,/. Mrs./PROPN Gordon/PROPN Blumberg/PROPN ,/. Mrs./PROPN Oscar/PROPN Bregman/PROPN ,/. Mrs./PROPN Alfred/PROPN Kershbaum/PROPN and/CCONJ Mrs./PROPN Edward/PROPN Sabol/PROPN ./.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Looking at the longest sentence\n",
    "pos_patt_brown1862=pos_pattern(sent_df.sentence.iloc[1862])\n",
    "pos_spa_brown1862=pos_spacy(sent_df.sentence.iloc[1862])\n",
    "compare_pos(pos_patt_brown1862,pos_spa_brown1862,\"patternVsspacy\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy POS tagger :  False\n"
     ]
    }
   ],
   "source": [
    "print(\"spacy POS tagger : \", sent_df.spacy_pos_tagger.iloc[1862])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">From the above comparison, we can see that **Titles** have been identified as a \"NOUN\" by Pattern but a \"PROPN\" by spaCy. This sentence was a match with Brown corpus for pattern and is a mismatch for spaCy.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Short incorrect sentence at index# 57077_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tag may not be correct as it does not matches the golden tag\n",
      "golden tag ---:\n",
      "We/PRON could/VERB n't/ADV help/VERB laughing/VERB ./.\n",
      "POS tag --- patternVsspacy :\n",
      "We/PRON could/AUX n't/ADV help/VERB laughing/VERB ./.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Looking at the longest sentence\n",
    "pos_patt_brown57077=pos_pattern(sent_df.sentence.iloc[57077])\n",
    "pos_spa_brown57077=pos_spacy(sent_df.sentence.iloc[57077])\n",
    "compare_pos(pos_patt_brown57077,pos_spa_brown57077,\"patternVsspacy\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('We', 'PR'),\n",
       " ('could', 'VB'),\n",
       " (\"n't\", 'RB'),\n",
       " ('help', 'VB'),\n",
       " ('laughing', 'VB'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag(sent_df.sentence.iloc[57077],tagset=UNIVERSAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Auxillery verb is a part of the Universal dataset, but Pattern recognized it as a Verb and spaCy correctly identified it as an auxillery. In this part that is the only difference.\n",
    "Pattern function ran without any failure for missing corresponding word in the dictionary for \"AUX\" , meaning it did not identify any Auxillery words.\n",
    "To avoid mismatches due to combined tags, function is splitting on first hyphen considering first part of a combined tag. Ran the original pattern tag function and it has identified could as just a \"verb\" and does not include any combined tag.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**Pattern returned POS tags and spaCy returned POS tags , using the universal tagset , produces different results for the 2 sentences compared**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tag is correct and matches the golden tag\n",
      "golden tag ---:\n",
      "Also/RB Mrs./NNP Berton/NNP Korman/NNP ,/, Mrs./NNP Morton/NNP Rosen/NNP ,/, Mrs./NNP Jacques/NNP Zinman/NNP ,/, Mrs./NNP Evelyn/NNP Rosen/NNP ,/, Mrs./NNP Henry/NNP Schultz/NNP ,/, Mr./NNP and/CC Mrs./NNP I./NNP S./NNP Kamens/NNP ,/, Mrs./NNP Jack/NNP Langsdorf/NNP ,/, Mrs./NNP Leonard/NNP Liss/NNP ,/, Mrs./NNP Gordon/NNP Blumberg/NNP ,/, Mrs./NNP Oscar/NNP Bregman/NNP ,/, Mrs./NNP Alfred/NNP Kershbaum/NNP and/CC Mrs./NNP Edward/NNP Sabol/NNP ./.\n",
      "POS tag --- patternVsspacy :\n",
      "Also/RB Mrs./NNP Berton/NNP Korman/NNP ,/, Mrs./NNP Morton/NNP Rosen/NNP ,/, Mrs./NNP Jacques/NNP Zinman/NNP ,/, Mrs./NNP Evelyn/NNP Rosen/NNP ,/, Mrs./NNP Henry/NNP Schultz/NNP ,/, Mr./NNP and/CC Mrs./NNP I./NNP S./NNP Kamens/NNP ,/, Mrs./NNP Jack/NNP Langsdorf/NNP ,/, Mrs./NNP Leonard/NNP Liss/NNP ,/, Mrs./NNP Gordon/NNP Blumberg/NNP ,/, Mrs./NNP Oscar/NNP Bregman/NNP ,/, Mrs./NNP Alfred/NNP Kershbaum/NNP and/CC Mrs./NNP Edward/NNP Sabol/NNP ./.\n",
      "\n",
      "\n",
      "POS tag is correct and matches the golden tag\n",
      "golden tag ---:\n",
      "We/PRP could/MD n't/RB help/VB laughing/VBG ./.\n",
      "POS tag --- patternVsspacy :\n",
      "We/PRP could/MD n't/RB help/VB laughing/VBG ./.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Looking at the longest sentence\n",
    "pos_patt_penn_brown1862=pos_pattern_penn(sent_df.sentence.iloc[1862])\n",
    "pos_spa_penn_brown1862=pos_spacy_penn(sent_df.sentence.iloc[1862])\n",
    "compare_pos(pos_patt_penn_brown1862,pos_spa_penn_brown1862,\"patternVsspacy\", True)\n",
    "print()\n",
    "print()\n",
    "## Looking at the longest sentence\n",
    "pos_patt_penn_brown57077=pos_pattern_penn(sent_df.sentence.iloc[57077])\n",
    "pos_spa_penn_brown57077=pos_spacy_penn(sent_df.sentence.iloc[57077])\n",
    "compare_pos(pos_patt_penn_brown57077,pos_spa_penn_brown57077,\"patternVsspacy\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**Pattern returned POS tags and spaCy returned POS tags , using the Penn-Tag tagset , produces same results for the 2 sentences compared**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exppatternvsspacy\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "\n",
    "### Explain any differences between Pattern POS tagger and spaCy POS tagger\n",
    "\n",
    "For starters :\n",
    "1. Pattern universal POS tagger returned 4646 matching tags while spaCy universal POS tagger returned 8978 matching tags when compared to Brown corpus  \n",
    "2. Pattern uses Universal tagset while spaCy uses Google's Universal tagset which has couple more additions  \n",
    "3. Pattern was faster to process the 57340 sentences than spaCy\n",
    "4. For the sentences compared, there were differences in the universal tags returned by pattern and spacy but their tags matched when using the Penn-Tag tagset. Which means there could be more matches between pattern and spacy using the Penn-Tag tagset\n",
    "5. Pattern did not identify Auxillery verbs differently than Verbs , although AUX is part of the universal tagset.\n",
    "6. spaCy identified Titles e.g. 'Mrs' as a ProperNoun while pattern identified it as a Noun as the humanists for the Brown Corpus did"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"news\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "### News article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "news=\"Boosted by a weaker dollar and fresh enthusiasm for cryptocurrencies, bitcoin surged past $10,000 for the first time in a year.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nenewsmanualws\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "\n",
    "Manually create POS tags using the Penn-Tag tagset. Trying my best here.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_manual=[('Boosted', 'VB'),\n",
    " ('by', 'IN'),\n",
    " ('a', 'DT'),\n",
    " ('weaker', 'JJR'),\n",
    " ('dollar', 'NN'),\n",
    " ('and', 'CC'),\n",
    " ('fresh', 'JJ'),\n",
    " ('enthusiasm', 'NN'),\n",
    " ('for', 'IN'),\n",
    " ('cryptocurrencies', 'NN'),\n",
    " (',', ','),\n",
    " ('bitcoin', 'NN'),\n",
    " ('surged', 'VB'),\n",
    " ('past', 'IN'),\n",
    " ('$', '$'),\n",
    " ('10,000', 'CD'),\n",
    " ('for', 'IN'),\n",
    " ('the', 'DT'),\n",
    " ('first', 'CD'),\n",
    " ('time', 'NN'),\n",
    " ('in', 'IN'),\n",
    " ('a', 'DT'),\n",
    " ('year', 'NN'),\n",
    " ('.', '.')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"newscomppatternvsspacy\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "\n",
    "### Compare Manual POS tags vs Pattern POS tags vs SpaCy POS tags using Penn-Tag tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_patt=pos_pattern_penn(news)\n",
    "pos_spac=pos_spacy_penn(news)\n",
    "news_pos_df=pd.DataFrame({\"manual\":pos_manual,\n",
    "                         \"pattern\":pos_patt,\n",
    "                         \"spacy\":pos_spac})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphical view of the SpaCy POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanviarora/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html><html lang=\"en\"><head><title>displaCy</title></head><body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\"><figure style=\"margin-bottom: 6rem\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"2b9936a252f948d3ba1cfe6cd8b92d70-0\" class=\"displacy\" width=\"3350\" height=\"512.0\" direction=\"ltr\" style=\"max-width: none; height: 512.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\"><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"422.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Boosted</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">VERB</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"422.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"200\">by</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"200\">ADP</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"422.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">a</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">DET</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"422.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">weaker</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">ADJ</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"422.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">dollar</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">NOUN</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"422.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"800\">and</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"800\">CCONJ</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"422.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">fresh</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">ADJ</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"422.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">enthusiasm</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"422.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1250\">for</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1250\">ADP</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"422.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1400\">cryptocurrencies,</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1400\">NOUN</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"422.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1550\">bitcoin</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1550\">NOUN</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"422.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1700\">surged</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1700\">VERB</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"422.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1850\">past</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1850\">ADP</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"422.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2000\">$</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2000\">SYM</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"422.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">10,000</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">NUM</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"422.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2300\">for</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2300\">ADP</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"422.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2450\">the</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2450\">DET</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"422.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2600\">first</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2600\">ADJ</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"422.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2750\">time</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2750\">NOUN</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"422.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2900\">in</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2900\">ADP</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"422.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3050\">a</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3050\">DET</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"422.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3200\">year.</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3200\">NOUN</tspan></text><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-0\" stroke-width=\"2px\" d=\"M62,377.0 62,252.0 1700.0,252.0 1700.0,377.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath></text><path class=\"displacy-arrowhead\" d=\"M62,379.0 L58,371.0 66,371.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-1\" stroke-width=\"2px\" d=\"M62,377.0 62,352.0 188.0,352.0 188.0,377.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">agent</textPath></text><path class=\"displacy-arrowhead\" d=\"M188.0,379.0 L192.0,371.0 184.0,371.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-2\" stroke-width=\"2px\" d=\"M362,377.0 362,327.0 641.0,327.0 641.0,377.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath></text><path class=\"displacy-arrowhead\" d=\"M362,379.0 L358,371.0 366,371.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-3\" stroke-width=\"2px\" d=\"M512,377.0 512,352.0 638.0,352.0 638.0,377.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath></text><path class=\"displacy-arrowhead\" d=\"M512,379.0 L508,371.0 516,371.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-4\" stroke-width=\"2px\" d=\"M212,377.0 212,302.0 644.0,302.0 644.0,377.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath></text><path class=\"displacy-arrowhead\" d=\"M644.0,379.0 L648.0,371.0 640.0,371.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-5\" stroke-width=\"2px\" d=\"M662,377.0 662,352.0 788.0,352.0 788.0,377.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath></text><path class=\"displacy-arrowhead\" d=\"M788.0,379.0 L792.0,371.0 784.0,371.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-6\" stroke-width=\"2px\" d=\"M962,377.0 962,352.0 1088.0,352.0 1088.0,377.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath></text><path class=\"displacy-arrowhead\" d=\"M962,379.0 L958,371.0 966,371.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-7\" stroke-width=\"2px\" d=\"M662,377.0 662,302.0 1094.0,302.0 1094.0,377.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath></text><path class=\"displacy-arrowhead\" d=\"M1094.0,379.0 L1098.0,371.0 1090.0,371.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-8\" stroke-width=\"2px\" d=\"M1112,377.0 1112,352.0 1238.0,352.0 1238.0,377.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath></text><path class=\"displacy-arrowhead\" d=\"M1238.0,379.0 L1242.0,371.0 1234.0,371.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-9\" stroke-width=\"2px\" d=\"M1262,377.0 1262,352.0 1388.0,352.0 1388.0,377.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath></text><path class=\"displacy-arrowhead\" d=\"M1388.0,379.0 L1392.0,371.0 1384.0,371.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-10\" stroke-width=\"2px\" d=\"M1562,377.0 1562,352.0 1688.0,352.0 1688.0,377.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath></text><path class=\"displacy-arrowhead\" d=\"M1562,379.0 L1558,371.0 1566,371.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-11\" stroke-width=\"2px\" d=\"M1862,377.0 1862,327.0 2141.0,327.0 2141.0,377.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath></text><path class=\"displacy-arrowhead\" d=\"M1862,379.0 L1858,371.0 1866,371.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-12\" stroke-width=\"2px\" d=\"M2012,377.0 2012,352.0 2138.0,352.0 2138.0,377.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath></text><path class=\"displacy-arrowhead\" d=\"M2012,379.0 L2008,371.0 2016,371.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-13\" stroke-width=\"2px\" d=\"M1712,377.0 1712,302.0 2144.0,302.0 2144.0,377.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath></text><path class=\"displacy-arrowhead\" d=\"M2144.0,379.0 L2148.0,371.0 2140.0,371.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-14\" stroke-width=\"2px\" d=\"M1712,377.0 1712,277.0 2297.0,277.0 2297.0,377.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath></text><path class=\"displacy-arrowhead\" d=\"M2297.0,379.0 L2301.0,371.0 2293.0,371.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-15\" stroke-width=\"2px\" d=\"M2462,377.0 2462,327.0 2741.0,327.0 2741.0,377.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath></text><path class=\"displacy-arrowhead\" d=\"M2462,379.0 L2458,371.0 2466,371.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-16\" stroke-width=\"2px\" d=\"M2612,377.0 2612,352.0 2738.0,352.0 2738.0,377.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath></text><path class=\"displacy-arrowhead\" d=\"M2612,379.0 L2608,371.0 2616,371.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-17\" stroke-width=\"2px\" d=\"M2312,377.0 2312,302.0 2744.0,302.0 2744.0,377.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath></text><path class=\"displacy-arrowhead\" d=\"M2744.0,379.0 L2748.0,371.0 2740.0,371.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-18\" stroke-width=\"2px\" d=\"M2762,377.0 2762,352.0 2888.0,352.0 2888.0,377.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-18\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath></text><path class=\"displacy-arrowhead\" d=\"M2888.0,379.0 L2892.0,371.0 2884.0,371.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-19\" stroke-width=\"2px\" d=\"M3062,377.0 3062,352.0 3188.0,352.0 3188.0,377.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-19\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath></text><path class=\"displacy-arrowhead\" d=\"M3062,379.0 L3058,371.0 3066,371.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-20\" stroke-width=\"2px\" d=\"M2912,377.0 2912,327.0 3191.0,327.0 3191.0,377.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-2b9936a252f948d3ba1cfe6cd8b92d70-0-20\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath></text><path class=\"displacy-arrowhead\" d=\"M3191.0,379.0 L3195.0,371.0 3187.0,371.0\" fill=\"currentColor\"/></g></svg></figure></body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'dep' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "pos_spac_viz=sp_nlp(news)\n",
    "sentence_spans=list(pos_spac_viz.sents)\n",
    "displacy.serve(sentence_spans, style=\"dep\", minify=True, options={\"compact\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare 2 tags at a time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tag may not be correct as it does not matches the golden tag\n",
      "golden tag ---:\n",
      "Boosted/VB by/IN a/DT weaker/JJR dollar/NN and/CC fresh/JJ enthusiasm/NN for/IN cryptocurrencies/NN ,/, bitcoin/NN surged/VB past/IN $/$ 10,000/CD for/IN the/DT first/CD time/NN in/IN a/DT year/NN ./.\n",
      "POS tag --- manualVspattern :\n",
      "Boosted/VBD by/IN a/DT weaker/JJR dollar/NN and/CC fresh/JJ enthusiasm/NN for/IN cryptocurrencies/NNS ,/, bitcoin/NN surged/VBD past/RB $/$ 10,000/CD for/IN the/DT first/JJ time/NN in/IN a/DT year/NN ./.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_pos(pos_manual,pos_patt,\"manualVspattern\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tag may not be correct as it does not matches the golden tag\n",
      "golden tag ---:\n",
      "Boosted/VB by/IN a/DT weaker/JJR dollar/NN and/CC fresh/JJ enthusiasm/NN for/IN cryptocurrencies/NN ,/, bitcoin/NN surged/VB past/IN $/$ 10,000/CD for/IN the/DT first/CD time/NN in/IN a/DT year/NN ./.\n",
      "POS tag --- manualVsspacy :\n",
      "Boosted/VBN by/IN a/DT weaker/JJR dollar/NN and/CC fresh/JJ enthusiasm/NN for/IN cryptocurrencies/NNS ,/, bitcoin/NN surged/VBD past/IN $/$ 10,000/CD for/IN the/DT first/JJ time/NN in/IN a/DT year/NN ./.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_pos(pos_manual,pos_spac,\"manualVsspacy\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tag may not be correct as it does not matches the golden tag\n",
      "golden tag ---:\n",
      "Boosted/VBD by/IN a/DT weaker/JJR dollar/NN and/CC fresh/JJ enthusiasm/NN for/IN cryptocurrencies/NNS ,/, bitcoin/NN surged/VBD past/RB $/$ 10,000/CD for/IN the/DT first/JJ time/NN in/IN a/DT year/NN ./.\n",
      "POS tag --- patternVsspacy :\n",
      "Boosted/VBN by/IN a/DT weaker/JJR dollar/NN and/CC fresh/JJ enthusiasm/NN for/IN cryptocurrencies/NNS ,/, bitcoin/NN surged/VBD past/IN $/$ 10,000/CD for/IN the/DT first/JJ time/NN in/IN a/DT year/NN ./.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_pos(pos_patt,pos_spac,\"patternVsspacy\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manual</th>\n",
       "      <th>pattern</th>\n",
       "      <th>spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Boosted, VB)</td>\n",
       "      <td>(Boosted, VBD)</td>\n",
       "      <td>(Boosted, VBN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(by, IN)</td>\n",
       "      <td>(by, IN)</td>\n",
       "      <td>(by, IN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(a, DT)</td>\n",
       "      <td>(a, DT)</td>\n",
       "      <td>(a, DT)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(weaker, JJR)</td>\n",
       "      <td>(weaker, JJR)</td>\n",
       "      <td>(weaker, JJR)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(dollar, NN)</td>\n",
       "      <td>(dollar, NN)</td>\n",
       "      <td>(dollar, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(and, CC)</td>\n",
       "      <td>(and, CC)</td>\n",
       "      <td>(and, CC)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(fresh, JJ)</td>\n",
       "      <td>(fresh, JJ)</td>\n",
       "      <td>(fresh, JJ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(enthusiasm, NN)</td>\n",
       "      <td>(enthusiasm, NN)</td>\n",
       "      <td>(enthusiasm, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(for, IN)</td>\n",
       "      <td>(for, IN)</td>\n",
       "      <td>(for, IN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(cryptocurrencies, NN)</td>\n",
       "      <td>(cryptocurrencies, NNS)</td>\n",
       "      <td>(cryptocurrencies, NNS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(,, ,)</td>\n",
       "      <td>(,, ,)</td>\n",
       "      <td>(,, ,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(bitcoin, NN)</td>\n",
       "      <td>(bitcoin, NN)</td>\n",
       "      <td>(bitcoin, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(surged, VB)</td>\n",
       "      <td>(surged, VBD)</td>\n",
       "      <td>(surged, VBD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(past, IN)</td>\n",
       "      <td>(past, RB)</td>\n",
       "      <td>(past, IN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>($, $)</td>\n",
       "      <td>($, $)</td>\n",
       "      <td>($, $)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(10,000, CD)</td>\n",
       "      <td>(10,000, CD)</td>\n",
       "      <td>(10,000, CD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(for, IN)</td>\n",
       "      <td>(for, IN)</td>\n",
       "      <td>(for, IN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(the, DT)</td>\n",
       "      <td>(the, DT)</td>\n",
       "      <td>(the, DT)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(first, CD)</td>\n",
       "      <td>(first, JJ)</td>\n",
       "      <td>(first, JJ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(time, NN)</td>\n",
       "      <td>(time, NN)</td>\n",
       "      <td>(time, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(in, IN)</td>\n",
       "      <td>(in, IN)</td>\n",
       "      <td>(in, IN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(a, DT)</td>\n",
       "      <td>(a, DT)</td>\n",
       "      <td>(a, DT)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(year, NN)</td>\n",
       "      <td>(year, NN)</td>\n",
       "      <td>(year, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(., .)</td>\n",
       "      <td>(., .)</td>\n",
       "      <td>(., .)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    manual                  pattern                    spacy\n",
       "0            (Boosted, VB)           (Boosted, VBD)           (Boosted, VBN)\n",
       "1                 (by, IN)                 (by, IN)                 (by, IN)\n",
       "2                  (a, DT)                  (a, DT)                  (a, DT)\n",
       "3            (weaker, JJR)            (weaker, JJR)            (weaker, JJR)\n",
       "4             (dollar, NN)             (dollar, NN)             (dollar, NN)\n",
       "5                (and, CC)                (and, CC)                (and, CC)\n",
       "6              (fresh, JJ)              (fresh, JJ)              (fresh, JJ)\n",
       "7         (enthusiasm, NN)         (enthusiasm, NN)         (enthusiasm, NN)\n",
       "8                (for, IN)                (for, IN)                (for, IN)\n",
       "9   (cryptocurrencies, NN)  (cryptocurrencies, NNS)  (cryptocurrencies, NNS)\n",
       "10                  (,, ,)                   (,, ,)                   (,, ,)\n",
       "11           (bitcoin, NN)            (bitcoin, NN)            (bitcoin, NN)\n",
       "12            (surged, VB)            (surged, VBD)            (surged, VBD)\n",
       "13              (past, IN)               (past, RB)               (past, IN)\n",
       "14                  ($, $)                   ($, $)                   ($, $)\n",
       "15            (10,000, CD)             (10,000, CD)             (10,000, CD)\n",
       "16               (for, IN)                (for, IN)                (for, IN)\n",
       "17               (the, DT)                (the, DT)                (the, DT)\n",
       "18             (first, CD)              (first, JJ)              (first, JJ)\n",
       "19              (time, NN)               (time, NN)               (time, NN)\n",
       "20                (in, IN)                 (in, IN)                 (in, IN)\n",
       "21                 (a, DT)                  (a, DT)                  (a, DT)\n",
       "22              (year, NN)               (year, NN)               (year, NN)\n",
       "23                  (., .)                   (., .)                   (., .)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_rows = 25\n",
    "news_pos_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"newsexppatternvsspacy\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "\n",
    "### Explain any differences between Manual POS tagger ,Pattern POS tagger and spaCy POS tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the compare function we know that the 3 tags are different. Manually analyzing them, we see that there are few differences. Let's discuss them below :\n",
    "\n",
    "1. Boosted : I have plainly mentioned it as a 'VB'. Penn-Tag tagset offers more varieties to verbs and probably I used it wrong here. But pattern recognizes it as a 'VBD' i.e. \"verb,past tense\" and spaCy recognized it as 'VBN' , i.e. \"verb.past participle\" . \n",
    "2. cryptocurrencies : Again I have just considered it as a Noun(NN), but applying the varieties offered by Penn-Tag tagset, both pattern and spacy match it as 'NNS' which is \"Noun Plural\"\n",
    "3. surged : 'VB' i.e Verb marked by me is 'VBD' i.e. \"verb,past tense\" by both pattern and spaCy\n",
    "4. first : Incorrectly tagged by me as a 'CD' i.e. \"Cardinal\" , is clearly an adjective as correctly marked by both pattern and spacy as it describes the noun 'time'\n",
    "\n",
    "<span style=\"color:blue\">**Overall looks like we have 4 differences , considering the tagset used, all 4 incorrectly tagged by me.\n",
    "Pattern and spaCy on the other hand have only 1 difference for the word 'Boosted' , sounds like pattern could be a winner here with verb,past tense**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What worked and What did not ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First attempt was made to compare Brown corpus tags in default/PennTag tagset with Pattern's default PennTag tagset. This returned the longest sentence lenght of matching tags as only 9. Further analysis pointed out that Pattern's PennTag tagger was considering a double quote(\") as 2 single quotes('). This is definitely something to look out for. Tried some logic for ignoring punctuation characters in this case but it requires more work. Not presented in this book\n",
    "2. PenTagg tagset returned by brown corpus, has a few combined tags. These combined tags are used differently. Tried splitting the tags on hyphen \"-\" that joins a combined tag, but still same result, longest matching sentence returned was of 9 words.\n",
    "3. Finally used UNIVERSAL tagset to compare the tags automatically. This yielded better results , but required use of dictionary as the outputs in pattern and spacy were slightly different.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"addtagers\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "### Additional POS taggers ??\n",
    "\n",
    "#### NLTK ClassiferBasedPOSTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "from nltk.corpus import treebank,brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=brown.tagged_sents(tagset='universal')\n",
    "cbpt_tagging = ClassifierBasedPOSTagger(train = train_data)\n",
    "\n",
    "def pos_cbpt_uni(tokens):\n",
    "    return cbpt_tagging.tag(tokens)\n",
    "\n",
    "## returns spaCy POS tags for the sentence at a given index along with its comparision with golden tagset\n",
    "def pos_cbpt_disp_byindex(ind,golden_tag_lst,input_df,pos_tag_name):\n",
    "    cbpt_pos_ind=pos_cbpt_uni(input_df.tokens.iloc[ind])\n",
    "    brown_golden_ind=golden_tag_lst[ind]\n",
    "    print(\"original sentence --- :\")\n",
    "    print(input_df.sentence.iloc[ind])\n",
    "    compare_pos(brown_golden_ind,cbpt_pos_ind,pos_tag_name,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbpt_pos=[]\n",
    "result_cb_pos=[]\n",
    "for index in range(0,len(sent_df)):\n",
    "    #print(\"index :\",index)\n",
    "    cb_s=pos_cbpt_uni(sent_df.tokens.iloc[index])\n",
    "    #print(s_p)\n",
    "    cbpt_pos.append(cb_s)\n",
    "    result_cb_pos.append(compare_pos(brown_goldentag[sent_df.position.iloc[index]],cb_s,'cbpt_pos',False))\n",
    "\n",
    "sent_df['cbpt_pos_tagger']=result_cb_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>length</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sentence</th>\n",
       "      <th>pattern_pos_tagger</th>\n",
       "      <th>spacy_pos_tagger</th>\n",
       "      <th>cbpt_pos_tagger</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33619</th>\n",
       "      <td>33619</td>\n",
       "      <td>82</td>\n",
       "      <td>[The, great, majority, of, present-day, lingui...</td>\n",
       "      <td>The great majority of present-day linguists fa...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20159</th>\n",
       "      <td>20159</td>\n",
       "      <td>80</td>\n",
       "      <td>[Our, leadership, in, a, wide, economic, boyco...</td>\n",
       "      <td>Our leadership in a wide economic boycott of S...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26626</th>\n",
       "      <td>26626</td>\n",
       "      <td>78</td>\n",
       "      <td>[But, his, rancor, did, not, cease, ,, and, pr...</td>\n",
       "      <td>But his rancor did not cease , and presently ,...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29684</th>\n",
       "      <td>29684</td>\n",
       "      <td>77</td>\n",
       "      <td>[For, this, reason, ,, the, more, uncertain, s...</td>\n",
       "      <td>For this reason , the more uncertain skywave s...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35766</th>\n",
       "      <td>35766</td>\n",
       "      <td>75</td>\n",
       "      <td>[To, derive, Utopian, communism, from, the, Je...</td>\n",
       "      <td>To derive Utopian communism from the Jerusalem...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32220</th>\n",
       "      <td>32220</td>\n",
       "      <td>75</td>\n",
       "      <td>[For, an, experiment, to, qualify, as, a, bino...</td>\n",
       "      <td>For an experiment to qualify as a binomial exp...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6370</th>\n",
       "      <td>6370</td>\n",
       "      <td>74</td>\n",
       "      <td>[The, Illinois, Commission, for, Handicapped, ...</td>\n",
       "      <td>The Illinois Commission for Handicapped Childr...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53056</th>\n",
       "      <td>53056</td>\n",
       "      <td>73</td>\n",
       "      <td>[She, glanced, at, the, man, nodding, beside, ...</td>\n",
       "      <td>She glanced at the man nodding beside her , a ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13117</th>\n",
       "      <td>13117</td>\n",
       "      <td>73</td>\n",
       "      <td>[The, drill, press, consists, of, a, vertical,...</td>\n",
       "      <td>The drill press consists of a vertical shaft (...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16016</th>\n",
       "      <td>16016</td>\n",
       "      <td>72</td>\n",
       "      <td>[Such, understanding, helps, to, explain, why,...</td>\n",
       "      <td>Such understanding helps to explain why one ma...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35379</th>\n",
       "      <td>35379</td>\n",
       "      <td>71</td>\n",
       "      <td>[Obviously, ,, a, satisfactory, answer, to, th...</td>\n",
       "      <td>Obviously , a satisfactory answer to the third...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35428</th>\n",
       "      <td>35428</td>\n",
       "      <td>71</td>\n",
       "      <td>[If, the, patient, can, perceive, figure, kine...</td>\n",
       "      <td>If the patient can perceive figure kinesthetic...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30882</th>\n",
       "      <td>30882</td>\n",
       "      <td>1</td>\n",
       "      <td>[Introduction]</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14626</th>\n",
       "      <td>14626</td>\n",
       "      <td>1</td>\n",
       "      <td>[Holidays]</td>\n",
       "      <td>Holidays</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7512</th>\n",
       "      <td>7512</td>\n",
       "      <td>1</td>\n",
       "      <td>[Confrontation]</td>\n",
       "      <td>Confrontation</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30723</th>\n",
       "      <td>30723</td>\n",
       "      <td>1</td>\n",
       "      <td>[samples]</td>\n",
       "      <td>samples</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14643</th>\n",
       "      <td>14643</td>\n",
       "      <td>1</td>\n",
       "      <td>[Vacations]</td>\n",
       "      <td>Vacations</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30678</th>\n",
       "      <td>30678</td>\n",
       "      <td>1</td>\n",
       "      <td>[Introduction]</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>308</td>\n",
       "      <td>1</td>\n",
       "      <td>[Oslo]</td>\n",
       "      <td>Oslo</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30481</th>\n",
       "      <td>30481</td>\n",
       "      <td>1</td>\n",
       "      <td>[Introduction]</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30473</th>\n",
       "      <td>30473</td>\n",
       "      <td>1</td>\n",
       "      <td>[Abstract]</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45124</th>\n",
       "      <td>45124</td>\n",
       "      <td>1</td>\n",
       "      <td>[Then]</td>\n",
       "      <td>Then</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30403</th>\n",
       "      <td>30403</td>\n",
       "      <td>1</td>\n",
       "      <td>[Introduction]</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11447</th>\n",
       "      <td>11447</td>\n",
       "      <td>1</td>\n",
       "      <td>[)]</td>\n",
       "      <td>)</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31360 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       position  length                                             tokens  \\\n",
       "33619     33619      82  [The, great, majority, of, present-day, lingui...   \n",
       "20159     20159      80  [Our, leadership, in, a, wide, economic, boyco...   \n",
       "26626     26626      78  [But, his, rancor, did, not, cease, ,, and, pr...   \n",
       "29684     29684      77  [For, this, reason, ,, the, more, uncertain, s...   \n",
       "35766     35766      75  [To, derive, Utopian, communism, from, the, Je...   \n",
       "32220     32220      75  [For, an, experiment, to, qualify, as, a, bino...   \n",
       "6370       6370      74  [The, Illinois, Commission, for, Handicapped, ...   \n",
       "53056     53056      73  [She, glanced, at, the, man, nodding, beside, ...   \n",
       "13117     13117      73  [The, drill, press, consists, of, a, vertical,...   \n",
       "16016     16016      72  [Such, understanding, helps, to, explain, why,...   \n",
       "35379     35379      71  [Obviously, ,, a, satisfactory, answer, to, th...   \n",
       "35428     35428      71  [If, the, patient, can, perceive, figure, kine...   \n",
       "...         ...     ...                                                ...   \n",
       "30882     30882       1                                     [Introduction]   \n",
       "14626     14626       1                                         [Holidays]   \n",
       "7512       7512       1                                    [Confrontation]   \n",
       "30723     30723       1                                          [samples]   \n",
       "14643     14643       1                                        [Vacations]   \n",
       "30678     30678       1                                     [Introduction]   \n",
       "308         308       1                                             [Oslo]   \n",
       "30481     30481       1                                     [Introduction]   \n",
       "30473     30473       1                                         [Abstract]   \n",
       "45124     45124       1                                             [Then]   \n",
       "30403     30403       1                                     [Introduction]   \n",
       "11447     11447       1                                                [)]   \n",
       "\n",
       "                                                sentence  pattern_pos_tagger  \\\n",
       "33619  The great majority of present-day linguists fa...               False   \n",
       "20159  Our leadership in a wide economic boycott of S...               False   \n",
       "26626  But his rancor did not cease , and presently ,...               False   \n",
       "29684  For this reason , the more uncertain skywave s...               False   \n",
       "35766  To derive Utopian communism from the Jerusalem...               False   \n",
       "32220  For an experiment to qualify as a binomial exp...               False   \n",
       "6370   The Illinois Commission for Handicapped Childr...               False   \n",
       "53056  She glanced at the man nodding beside her , a ...               False   \n",
       "13117  The drill press consists of a vertical shaft (...               False   \n",
       "16016  Such understanding helps to explain why one ma...               False   \n",
       "35379  Obviously , a satisfactory answer to the third...               False   \n",
       "35428  If the patient can perceive figure kinesthetic...               False   \n",
       "...                                                  ...                 ...   \n",
       "30882                                       Introduction                True   \n",
       "14626                                           Holidays                True   \n",
       "7512                                       Confrontation                True   \n",
       "30723                                            samples                True   \n",
       "14643                                          Vacations                True   \n",
       "30678                                       Introduction                True   \n",
       "308                                                 Oslo                True   \n",
       "30481                                       Introduction                True   \n",
       "30473                                           Abstract               False   \n",
       "45124                                               Then                True   \n",
       "30403                                       Introduction                True   \n",
       "11447                                                  )                True   \n",
       "\n",
       "       spacy_pos_tagger  cbpt_pos_tagger  \n",
       "33619             False             True  \n",
       "20159             False             True  \n",
       "26626             False             True  \n",
       "29684             False             True  \n",
       "35766             False             True  \n",
       "32220             False             True  \n",
       "6370              False             True  \n",
       "53056             False             True  \n",
       "13117             False             True  \n",
       "16016             False             True  \n",
       "35379             False             True  \n",
       "35428             False             True  \n",
       "...                 ...              ...  \n",
       "30882              True             True  \n",
       "14626              True             True  \n",
       "7512               True             True  \n",
       "30723              True             True  \n",
       "14643              True             True  \n",
       "30678              True             True  \n",
       "308               False             True  \n",
       "30481              True             True  \n",
       "30473             False             True  \n",
       "45124              True             True  \n",
       "30403              True             True  \n",
       "11447              True             True  \n",
       "\n",
       "[31360 rows x 7 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_df[sent_df['cbpt_pos_tagger']==True].sort_values(by=['length'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**ClassiferBasedPOSTagger trained on brown corpus itself, returned 31360 matched tags**</span>\n",
    "\n",
    "**The longest matching sentence , when compared to brown corpus tags, has a length of 82 words and shortest not matching sentence is 1 word long**\n",
    "Definitely this is higher accuracy as compared to the other taggers, but we should not forget we trained this Tagger on brown corpus tagged words itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check output for the same sentences from Pattern "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original sentence --- :\n",
      "In repetitions of the experiment from couple to couple , the votes of the two persons in a couple probably agree more often than independence would imply , because couples who visit the museum together are more likely to have similar tastes than are a random pair of people drawn from the entire population of visitors .\n",
      "POS tag may not be correct as it does not matches the golden tag\n",
      "golden tag ---:\n",
      "In/ADP repetitions/NOUN of/ADP the/DET experiment/NOUN from/ADP couple/NOUN to/ADP couple/NOUN ,/. the/DET votes/NOUN of/ADP the/DET two/NUM persons/NOUN in/ADP a/DET couple/NOUN probably/ADV agree/VERB more/ADV often/ADV than/ADP independence/NOUN would/VERB imply/VERB ,/. because/ADP couples/NOUN who/PRON visit/VERB the/DET museum/NOUN together/ADV are/VERB more/ADV likely/ADJ to/PRT have/VERB similar/ADJ tastes/NOUN than/ADP are/VERB a/DET random/ADJ pair/NOUN of/ADP people/NOUN drawn/VERB from/ADP the/DET entire/ADJ population/NOUN of/ADP visitors/NOUN ./.\n",
      "POS tag --- ClassifierBased_pos :\n",
      "In/ADP repetitions/NOUN of/ADP the/DET experiment/NOUN from/ADP couple/NOUN to/PRT couple/NOUN ,/. the/DET votes/NOUN of/ADP the/DET two/NUM persons/NOUN in/ADP a/DET couple/NOUN probably/ADV agree/VERB more/ADV often/ADV than/ADP independence/NOUN would/VERB imply/ADV ,/. because/ADP couples/NOUN who/PRON visit/VERB the/DET museum/NOUN together/ADV are/VERB more/ADV likely/ADJ to/PRT have/VERB similar/ADJ tastes/NOUN than/ADP are/VERB a/DET random/ADJ pair/NOUN of/ADP people/NOUN drawn/VERB from/ADP the/DET entire/ADJ population/NOUN of/ADP visitors/NOUN ./.\n"
     ]
    }
   ],
   "source": [
    "pos_cbpt_disp_byindex(32265,brown_goldentag,sent_df,'ClassifierBased_pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">This was the longest sentence correctly tagged , when compared with Brown Corpus, by pattern and we checked output for the same with spaCy POS Tagger as well. Both the initial taggers tagged it correctly but our ClassifierBased Model failed to do so.</span>  \n",
    "<span style=\"color:blue\">The only error i see is for couple 'to' couple where as per Brown Corpus 'to' is AdPosition but as per our model it is a particle.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original sentence --- :\n",
      "We couldn't help laughing .\n",
      "POS tag is correct and matches the golden tag\n",
      "golden tag ---:\n",
      "We/PRON couldn't/VERB help/VERB laughing/VERB ./.\n",
      "POS tag --- ClassifierBased_pos :\n",
      "We/PRON couldn't/VERB help/VERB laughing/VERB ./.\n"
     ]
    }
   ],
   "source": [
    "pos_cbpt_disp_byindex(57077,brown_goldentag,sent_df,'ClassifierBased_pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original sentence --- :\n",
      "Electric power\n",
      "POS tag is correct and matches the golden tag\n",
      "golden tag ---:\n",
      "Electric/ADJ power/NOUN\n",
      "POS tag --- ClassifierBased_pos :\n",
      "Electric/ADJ power/NOUN\n"
     ]
    }
   ],
   "source": [
    "pos_cbpt_disp_byindex(35692,brown_goldentag,sent_df,'ClassifierBased_pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Both the above short sentences , where pattern failed to match correctly, our Classifierbased POS Tagger matched it correctly. Also to notice it did not break the word \"couldn't\" into 2 words like pattern and spacy. Infact it kept it as 1 word like the Brown Corpus</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
